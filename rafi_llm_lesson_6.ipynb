{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFeyr3-g9n2t",
        "outputId": "42dade45-1810-456c-e803-8f2acb03c4ac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  pid, fd = os.forkpty()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: flax in ./.local/lib/python3.10/site-packages (0.10.5)\n",
            "Requirement already satisfied: tensorflow in ./.local/lib/python3.10/site-packages (2.19.0)\n",
            "Requirement already satisfied: tensorflow_datasets in ./.local/lib/python3.10/site-packages (4.9.8)\n",
            "Requirement already satisfied: jax>=0.5.1 in ./.local/lib/python3.10/site-packages (from flax) (0.6.0)\n",
            "Requirement already satisfied: msgpack in ./.local/lib/python3.10/site-packages (from flax) (1.1.0)\n",
            "Requirement already satisfied: optax in ./.local/lib/python3.10/site-packages (from flax) (0.2.4)\n",
            "Requirement already satisfied: orbax-checkpoint in ./.local/lib/python3.10/site-packages (from flax) (0.11.12)\n",
            "Requirement already satisfied: tensorstore in ./.local/lib/python3.10/site-packages (from flax) (0.1.73)\n",
            "Requirement already satisfied: rich>=11.1 in ./.local/lib/python3.10/site-packages (from flax) (14.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.2 in /usr/local/lib/python3.10/dist-packages (from flax) (4.13.2)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/lib/python3/dist-packages (from flax) (5.4.1)\n",
            "Requirement already satisfied: treescope>=0.1.7 in ./.local/lib/python3.10/site-packages (from flax) (0.1.9)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in ./.local/lib/python3.10/site-packages (from tensorflow) (2.2.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in ./.local/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in ./.local/lib/python3.10/site-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./.local/lib/python3.10/site-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in ./.local/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in ./.local/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in ./.local/lib/python3.10/site-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in ./.local/lib/python3.10/site-packages (from tensorflow) (4.21.12)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in ./.local/lib/python3.10/site-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in ./.local/lib/python3.10/site-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./.local/lib/python3.10/site-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in ./.local/lib/python3.10/site-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in ./.local/lib/python3.10/site-packages (from tensorflow) (3.9.2)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in ./.local/lib/python3.10/site-packages (from tensorflow) (2.1.3)\n",
            "Requirement already satisfied: h5py>=3.11.0 in ./.local/lib/python3.10/site-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in ./.local/lib/python3.10/site-packages (from tensorflow) (0.5.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./.local/lib/python3.10/site-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: array_record>=0.5.0 in ./.local/lib/python3.10/site-packages (from tensorflow_datasets) (0.7.1)\n",
            "Requirement already satisfied: dm-tree in ./.local/lib/python3.10/site-packages (from tensorflow_datasets) (0.1.9)\n",
            "Requirement already satisfied: etils>=1.6.0 in ./.local/lib/python3.10/site-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow_datasets) (1.12.2)\n",
            "Requirement already satisfied: immutabledict in ./.local/lib/python3.10/site-packages (from tensorflow_datasets) (4.2.1)\n",
            "Requirement already satisfied: promise in ./.local/lib/python3.10/site-packages (from tensorflow_datasets) (2.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (7.0.0)\n",
            "Requirement already satisfied: pyarrow in ./.local/lib/python3.10/site-packages (from tensorflow_datasets) (19.0.1)\n",
            "Requirement already satisfied: simple_parsing in ./.local/lib/python3.10/site-packages (from tensorflow_datasets) (0.1.7)\n",
            "Requirement already satisfied: tensorflow-metadata in ./.local/lib/python3.10/site-packages (from tensorflow_datasets) (1.17.1)\n",
            "Requirement already satisfied: toml in ./.local/lib/python3.10/site-packages (from tensorflow_datasets) (0.10.2)\n",
            "Requirement already satisfied: tqdm in ./.local/lib/python3.10/site-packages (from tensorflow_datasets) (4.67.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: einops in ./.local/lib/python3.10/site-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow_datasets) (0.8.1)\n",
            "Requirement already satisfied: fsspec in ./.local/lib/python3.10/site-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow_datasets) (2025.3.2)\n",
            "Requirement already satisfied: importlib_resources in ./.local/lib/python3.10/site-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow_datasets) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/lib/python3/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow_datasets) (1.0.0)\n",
            "Requirement already satisfied: jaxlib<=0.6.0,>=0.6.0 in ./.local/lib/python3.10/site-packages (from jax>=0.5.1->flax) (0.6.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in ./.local/lib/python3.10/site-packages (from jax>=0.5.1->flax) (1.15.2)\n",
            "Requirement already satisfied: namex in ./.local/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in ./.local/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorflow) (1.26.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorflow) (2020.6.20)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.local/lib/python3.10/site-packages (from rich>=11.1->flax) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax) (2.19.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in ./.local/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./.local/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in ./.local/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.10/dist-packages (from dm-tree->tensorflow_datasets) (25.3.0)\n",
            "Requirement already satisfied: chex>=0.1.87 in ./.local/lib/python3.10/site-packages (from optax->flax) (0.1.89)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (1.6.0)\n",
            "Requirement already satisfied: humanize in ./.local/lib/python3.10/site-packages (from orbax-checkpoint->flax) (4.12.2)\n",
            "Requirement already satisfied: simplejson>=3.16.0 in ./.local/lib/python3.10/site-packages (from orbax-checkpoint->flax) (3.20.1)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in ./.local/lib/python3.10/site-packages (from simple_parsing->tensorflow_datasets) (0.16)\n",
            "Requirement already satisfied: toolz>=0.9.0 in ./.local/lib/python3.10/site-packages (from chex>=0.1.87->optax->flax) (1.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in ./.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "\u001b[33mWARNING: Error parsing dependencies of distro-info: Invalid version: '1.1build1'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Error parsing dependencies of python-debian: Invalid version: '0.1.43ubuntu1'\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip install flax tensorflow tensorflow_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: jax[tpu] in ./.local/lib/python3.10/site-packages (0.6.0)\n",
            "Requirement already satisfied: jaxlib<=0.6.0,>=0.6.0 in ./.local/lib/python3.10/site-packages (from jax[tpu]) (0.6.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in ./.local/lib/python3.10/site-packages (from jax[tpu]) (0.5.1)\n",
            "Requirement already satisfied: numpy>=1.25 in ./.local/lib/python3.10/site-packages (from jax[tpu]) (2.1.3)\n",
            "Requirement already satisfied: opt_einsum in ./.local/lib/python3.10/site-packages (from jax[tpu]) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in ./.local/lib/python3.10/site-packages (from jax[tpu]) (1.15.2)\n",
            "Requirement already satisfied: libtpu==0.0.13.* in ./.local/lib/python3.10/site-packages (from jax[tpu]) (0.0.13)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from jax[tpu]) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->jax[tpu]) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->jax[tpu]) (3.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->jax[tpu]) (1.26.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->jax[tpu]) (2020.6.20)\n",
            "\u001b[33mWARNING: Error parsing dependencies of distro-info: Invalid version: '1.1build1'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Error parsing dependencies of python-debian: Invalid version: '0.1.43ubuntu1'\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip install \"jax[tpu]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
              " TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
              " TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
              " TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import jax\n",
        "jax.devices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## With Positional Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1Tu1Cjb-83aw"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.linen as nn\n",
        "from flax.training import train_state\n",
        "\n",
        "import functools\n",
        "\n",
        "import flax.linen.attention as attention\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import optax\n",
        "\n",
        "import time\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "Nve1Jxq_9bBD",
        "outputId": "04be3b44-6df1-44c0-92d5-102de758df0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9 -> 4.610074996948242 15.437712669372559\n",
            "19 -> 3.3865103721618652 0.5307536125183105\n",
            "29 -> 3.1280136108398438 0.5627996921539307\n",
            "39 -> 2.9500465393066406 0.5636236667633057\n",
            "49 -> 2.6155314445495605 0.5631210803985596\n",
            "59 -> 2.5496368408203125 0.565514087677002\n",
            "69 -> 2.5889205932617188 0.5621740818023682\n",
            "79 -> 2.585242748260498 0.5648152828216553\n",
            "89 -> 2.5686445236206055 0.5646486282348633\n",
            "99 -> 2.53019380569458 0.5624356269836426\n",
            "109 -> 2.8131237030029297 0.5647952556610107\n",
            "119 -> 2.562844753265381 0.5635154247283936\n",
            "129 -> 2.545961856842041 0.563058614730835\n",
            "139 -> 2.5722155570983887 0.5657546520233154\n",
            "149 -> 2.4941372871398926 0.5623538494110107\n",
            "159 -> 2.4459774494171143 0.5646297931671143\n",
            "169 -> 2.435457229614258 0.5642399787902832\n",
            "179 -> 2.4510440826416016 0.5650138854980469\n",
            "189 -> 2.4776196479797363 0.564751148223877\n",
            "199 -> 2.3942651748657227 0.5634090900421143\n",
            "209 -> 2.4091174602508545 0.5627098083496094\n",
            "219 -> 2.4573934078216553 0.5660290718078613\n",
            "229 -> 2.446984052658081 0.5634081363677979\n",
            "239 -> 2.435708999633789 0.5637948513031006\n",
            "249 -> 2.3479762077331543 0.5676102638244629\n",
            "259 -> 2.3003945350646973 0.5604872703552246\n",
            "269 -> 2.2875022888183594 0.5646226406097412\n",
            "279 -> 2.245006799697876 0.562821626663208\n",
            "289 -> 2.2315282821655273 0.5699102878570557\n",
            "299 -> 2.2357006072998047 0.5598101615905762\n",
            "309 -> 2.2218146324157715 0.5650444030761719\n",
            "319 -> 2.23176908493042 0.5615708827972412\n",
            "329 -> 2.1798315048217773 0.5651421546936035\n",
            "339 -> 2.1778981685638428 0.5638666152954102\n",
            "349 -> 2.162195920944214 0.5652616024017334\n",
            "359 -> 2.1672186851501465 0.5635979175567627\n",
            "369 -> 2.214766502380371 0.5650966167449951\n",
            "379 -> 2.123910427093506 0.564713716506958\n",
            "389 -> 2.151954412460327 0.5631434917449951\n",
            "399 -> 2.127481460571289 0.5639700889587402\n",
            "409 -> 2.116415023803711 0.5635712146759033\n",
            "419 -> 2.106780767440796 0.5657596588134766\n",
            "429 -> 2.0942070484161377 0.5637071132659912\n",
            "439 -> 2.090996265411377 0.5630612373352051\n",
            "449 -> 2.069653034210205 0.565300703048706\n",
            "459 -> 2.05043888092041 0.5634133815765381\n",
            "469 -> 2.0809006690979004 0.563427209854126\n",
            "479 -> 2.0834922790527344 0.5663573741912842\n",
            "489 -> 2.0724294185638428 0.5630648136138916\n",
            "499 -> 2.044102191925049 0.5657639503479004\n",
            "509 -> 2.042755603790283 0.5642056465148926\n",
            "519 -> 2.013944625854492 0.5646655559539795\n",
            "529 -> 2.092604637145996 0.5632059574127197\n",
            "539 -> 2.045232057571411 0.5658421516418457\n",
            "549 -> 2.0064358711242676 0.5628194808959961\n",
            "559 -> 2.0686700344085693 0.5639593601226807\n",
            "569 -> 2.0749402046203613 0.564061164855957\n",
            "579 -> 2.024496555328369 0.5644536018371582\n",
            "589 -> 2.0234932899475098 0.5646097660064697\n",
            "599 -> 2.038918972015381 0.5648090839385986\n",
            "609 -> 2.0468220710754395 0.5650951862335205\n",
            "619 -> 1.999839186668396 0.564244270324707\n",
            "629 -> 2.0433645248413086 0.5638720989227295\n",
            "639 -> 2.0606017112731934 0.5647625923156738\n",
            "649 -> 1.9953219890594482 0.5640275478363037\n",
            "659 -> 2.0823302268981934 0.5668435096740723\n",
            "669 -> 2.0894107818603516 0.5638260841369629\n",
            "679 -> 2.023714780807495 0.5607907772064209\n",
            "689 -> 2.0279293060302734 0.5658786296844482\n",
            "699 -> 2.049825429916382 0.5633387565612793\n",
            "709 -> 2.0249013900756836 0.5658092498779297\n",
            "719 -> 2.0338377952575684 0.5642871856689453\n",
            "729 -> 2.0126781463623047 0.5636181831359863\n",
            "739 -> 2.0312461853027344 0.5653789043426514\n",
            "749 -> 2.0071730613708496 0.5635647773742676\n",
            "759 -> 2.025027275085449 0.563621997833252\n",
            "769 -> 1.9799026250839233 0.5651829242706299\n",
            "779 -> 1.9632405042648315 0.5622527599334717\n",
            "789 -> 1.9993903636932373 0.5647258758544922\n",
            "799 -> 1.9598236083984375 0.5668344497680664\n",
            "809 -> 1.9787540435791016 0.5613102912902832\n",
            "819 -> 1.9547961950302124 0.5670008659362793\n",
            "829 -> 1.9637324810028076 0.5596296787261963\n",
            "839 -> 2.0170745849609375 0.5665290355682373\n",
            "849 -> 1.962158203125 0.5635766983032227\n",
            "859 -> 1.9357541799545288 0.5628700256347656\n",
            "869 -> 1.9240310192108154 0.5648820400238037\n",
            "879 -> 2.114438533782959 0.5635805130004883\n",
            "889 -> 1.9953784942626953 0.5637667179107666\n",
            "899 -> 1.9684031009674072 0.562915563583374\n",
            "909 -> 1.959874153137207 0.5653457641601562\n",
            "919 -> 2.0458054542541504 0.5639336109161377\n",
            "929 -> 2.0379960536956787 0.5628674030303955\n",
            "939 -> 1.9554470777511597 0.5641529560089111\n",
            "949 -> 1.985662817955017 0.5641443729400635\n",
            "959 -> 2.0141549110412598 0.5638291835784912\n",
            "969 -> 2.0029001235961914 0.5638065338134766\n",
            "979 -> 1.9507863521575928 0.5651004314422607\n",
            "989 -> 1.8753678798675537 0.5639908313751221\n",
            "999 -> 1.9545687437057495 0.5635302066802979\n"
          ]
        }
      ],
      "source": [
        "BATCH_IN_SEQUENCES = 384\n",
        "SEQUENCE_LENGTH = 128\n",
        "\n",
        "VOCAB_DIM = 256\n",
        "EMBED_DIM = 512\n",
        "FF_DIM = 2048\n",
        "\n",
        "NUM_HEADS = 4\n",
        "HEAD_DIM = 128\n",
        "\n",
        "LAYERS = 2\n",
        "\n",
        "HEAD_DEPTH = 128\n",
        "NUM_HEADS = 4\n",
        "\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "FSDP = 4 # 8 OR 4\n",
        "TENSOR = 1 # 1 OR 2\n",
        "\n",
        "\n",
        "def attention_ourselves(_Q, _K, _V):\n",
        "    _weights_unnormalized = jax.numpy.einsum(\"BSHD,BTHD->BHST\", _Q, _K)\n",
        "    _weights_unnormalized_to_zero_out = jax.numpy.triu( jax.numpy.ones((SEQUENCE_LENGTH,SEQUENCE_LENGTH), jax.numpy.bfloat16), 1)\n",
        "    _weights = jax.nn.softmax(_weights_unnormalized - 1e6 * _weights_unnormalized_to_zero_out)  ### Creating something of size (B,HEADS, SEQUENCE, SEQUENCE)\n",
        "    #print(f\"{_weights.size=}\")\n",
        "    output = jax.numpy.einsum(\"BHST,BTHD->BSHD\", _weights, _V)\n",
        "\n",
        "    return output\n",
        "\n",
        "class OurModel(nn.Module):\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    '''\n",
        "        x is [BATCH, SEQUENCE]\n",
        "    '''\n",
        "    embedding = self.param(\n",
        "        'embedding',\n",
        "        nn.with_partitioning(nn.initializers.normal(1), (\"tp\", \"fsdp\")),\n",
        "        (VOCAB_DIM, EMBED_DIM),\n",
        "        jnp.float32,\n",
        "    )\n",
        "    x = embedding[x] ##OUTPUT should be [BATCH, SEQUENCE, EMBED]\n",
        "\n",
        "    positional_embedding = self.param(\n",
        "        'positional_embedding',\n",
        "        nn.with_partitioning(nn.initializers.normal(1), (None, None, \"fsdp\")),\n",
        "        (1, SEQUENCE_LENGTH, EMBED_DIM),\n",
        "        jnp.float32,\n",
        "    )\n",
        "\n",
        "    x += positional_embedding\n",
        "\n",
        "\n",
        "    for i in range(LAYERS):\n",
        "      feedforward = self.param(\n",
        "          'feedforward_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('fsdp', 'tp')),\n",
        "          (EMBED_DIM, FF_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      x = x @ feedforward\n",
        "      x = jax.nn.relu(x)\n",
        "      embed = self.param(\n",
        "          'embed_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('tp', 'fsdp')),\n",
        "          (FF_DIM, EMBED_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      x = x @ embed\n",
        "      x = jax.nn.relu(x)\n",
        "\n",
        "      q_proj = self.param(\n",
        "          'qproj_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('fsdp', 'tp')),\n",
        "          (EMBED_DIM, NUM_HEADS, HEAD_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      q = jnp.einsum(\"BSE,EHD->BSHD\",x, q_proj )\n",
        "\n",
        "      k_proj = self.param(\n",
        "          'kproj_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('fsdp', 'tp')),\n",
        "          (EMBED_DIM, NUM_HEADS, HEAD_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      k = jnp.einsum(\"BSE,EHD->BSHD\",x, k_proj )\n",
        "\n",
        "      v_proj = self.param(\n",
        "          'vproj_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('fsdp', 'tp')),\n",
        "          (EMBED_DIM, NUM_HEADS, HEAD_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      v = jnp.einsum(\"BSE,EHD->BSHD\",x, v_proj )\n",
        "\n",
        "      o = attention_ourselves(q,k,v)\n",
        "\n",
        "      o_proj = self.param(\n",
        "          'oproj_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('fsdp', 'tp')),\n",
        "          (NUM_HEADS, HEAD_DIM, EMBED_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      x = jnp.einsum(\"BSHD,HDE->BSE\",o, o_proj )\n",
        "\n",
        "    return x @ embedding.T\n",
        "\n",
        "def convert_to_ascii(string_array, max_length):\n",
        "  result = np.zeros((len(string_array), max_length), dtype=np.uint8)\n",
        "  for i, string in enumerate(string_array):\n",
        "    for j, char in enumerate(string):\n",
        "      if j >= SEQUENCE_LENGTH:\n",
        "         break\n",
        "      result[i, j] = char\n",
        "  return result\n",
        "\n",
        "def input_to_output(np_array):\n",
        "   zero_array = np.zeros( (BATCH_IN_SEQUENCES,SEQUENCE_LENGTH), dtype = jnp.uint8)\n",
        "   zero_array[:, 1:SEQUENCE_LENGTH] = np_array[:, 0:SEQUENCE_LENGTH-1]\n",
        "   return zero_array\n",
        "\n",
        "def calculate_loss(params, model, inputs, outputs):\n",
        "   proposed_outputs = model.apply(params, inputs)\n",
        "   one_hot = jax.nn.one_hot(outputs, VOCAB_DIM)\n",
        "   loss = optax.softmax_cross_entropy(proposed_outputs, one_hot)\n",
        "   return jnp.mean(loss)\n",
        "\n",
        "\n",
        "def step(state, model, inputs, outputs):\n",
        "   loss, grad = jax.value_and_grad(calculate_loss)(state.params, model, inputs, outputs)\n",
        "   state = state.apply_gradients(grads = grad)\n",
        "   return loss, state\n",
        "\n",
        "\n",
        "mesh = jax.sharding.Mesh(np.reshape(  jax.devices(), (FSDP,TENSOR)), [\"fsdp\", \"tp\"])\n",
        "\n",
        "ds = tfds.load('lm1b', split='train', shuffle_files=False)\n",
        "ds = ds.batch(BATCH_IN_SEQUENCES)\n",
        "\n",
        "rngkey = jax.random.key(0)\n",
        "model = OurModel()\n",
        "\n",
        "shaped_init = jax.eval_shape( functools.partial(model.init, rngkey), jax.ShapeDtypeStruct((BATCH_IN_SEQUENCES, SEQUENCE_LENGTH), dtype = jnp.uint8))\n",
        "state_sharding = nn.get_sharding(shaped_init, mesh)\n",
        "_params = jax.jit(model.init, out_shardings = state_sharding)(rngkey, jax.ShapeDtypeStruct((BATCH_IN_SEQUENCES, SEQUENCE_LENGTH), dtype = jnp.uint8))\n",
        "\n",
        "tx = optax.adam(learning_rate = LEARNING_RATE)\n",
        "state = train_state.TrainState.create(\n",
        "    apply_fn = model.apply,\n",
        "    params = _params,\n",
        "    tx = tx\n",
        ")\n",
        "\n",
        "iter = 0\n",
        "static_step = jax.jit(step, static_argnums=1)\n",
        "\n",
        "last_step_time = time.time()\n",
        "stepnum = 0\n",
        "\n",
        "for example in ds:\n",
        "    outputs = convert_to_ascii(example['text'].numpy(), SEQUENCE_LENGTH)\n",
        "    inputs = input_to_output(outputs)\n",
        "\n",
        "    loss, state = static_step(state, model, inputs, outputs)\n",
        "    #loss, state = jax.jit(step, static_argnums=1)(state, model, inputs, outputs)\n",
        "    #loss, state = jax.jit(lambda x,y,z,a : step(x,y,z,a), static_argnums=1)(state, model, inputs, outputs)\n",
        "\n",
        "    stepnum += 1\n",
        "\n",
        "    if stepnum % 10 == 0:\n",
        "      new_time = time.time()\n",
        "      time_elapsed_seconds = (new_time-last_step_time)\n",
        "      last_step_time = new_time\n",
        "      print(f\"{iter} -> {loss} {time_elapsed_seconds}\")\n",
        "\n",
        "    if stepnum == 1000:\n",
        "       break\n",
        "\n",
        "    iter += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeG5iDNrisOl",
        "outputId": "10b0693c-ca32-41e7-edc2-35a77b454b88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 128, 256)\n",
            "logits 1 and 2 are the same: True\n",
            "logits 1 and 3 are NOT the same: True\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def predict(input_str, model, params):\n",
        "    input_tf = tf.constant([input_str], shape=(1,), dtype=tf.string)\n",
        "    input_ascii = convert_to_ascii(input_tf.numpy(), SEQUENCE_LENGTH)\n",
        "    logits = model.apply(params, input_ascii)\n",
        "    return logits\n",
        "\n",
        "# for example in ds:\n",
        "#     print(example[\"text\"][1:2])\n",
        "#     inputs = convert_to_ascii(example['text'][1:2].numpy(), SEQUENCE_LENGTH)\n",
        "#     break\n",
        "logits_1 = predict(\"test a\", model, state.params)\n",
        "logits_2 = predict(\"test a\", model, state.params)\n",
        "logits_3 = predict(\"a test\", model, state.params)\n",
        "print(logits_1.shape)\n",
        "print(\"logits 1 and 2 are the same:\", jnp.array_equal(logits_1, logits_2))\n",
        "print(\"logits 1 and 3 are NOT the same:\", not jnp.array_equal(logits_1, logits_3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Remove position embedding and rerun check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9 -> 3.7439961433410645 14.990861177444458\n",
            "19 -> 3.3444061279296875 0.524946928024292\n",
            "29 -> 2.9133424758911133 0.5640227794647217\n",
            "39 -> 2.644909381866455 0.5637977123260498\n",
            "49 -> 2.5924177169799805 0.5632658004760742\n",
            "59 -> 2.5294244289398193 0.5666470527648926\n",
            "69 -> 2.569258451461792 0.5604920387268066\n",
            "79 -> 2.562154531478882 0.5642075538635254\n",
            "89 -> 2.514817237854004 0.5647234916687012\n",
            "99 -> 2.442938804626465 0.5620982646942139\n",
            "109 -> 2.5612828731536865 0.5638775825500488\n",
            "119 -> 2.4192612171173096 0.5632226467132568\n",
            "129 -> 2.3162553310394287 0.5659706592559814\n",
            "139 -> 2.3402962684631348 0.563025951385498\n",
            "149 -> 2.225848436355591 0.5639653205871582\n",
            "159 -> 2.185987949371338 0.5623829364776611\n",
            "169 -> 2.1876444816589355 0.5629620552062988\n",
            "179 -> 2.204556465148926 0.5645332336425781\n",
            "189 -> 2.2342259883880615 0.5638141632080078\n",
            "199 -> 2.1401021480560303 0.5638136863708496\n",
            "209 -> 2.1552047729492188 0.5641112327575684\n",
            "219 -> 2.2246217727661133 0.5627484321594238\n",
            "229 -> 2.161456823348999 0.5646481513977051\n",
            "239 -> 2.163254737854004 0.5642745494842529\n",
            "249 -> 2.1357760429382324 0.5644004344940186\n",
            "259 -> 2.110314130783081 0.5629086494445801\n",
            "269 -> 2.121706008911133 0.5642471313476562\n",
            "279 -> 2.0796966552734375 0.5624279975891113\n",
            "289 -> 2.080627202987671 0.5657377243041992\n",
            "299 -> 2.099212408065796 0.5630667209625244\n",
            "309 -> 2.0810227394104004 0.5629281997680664\n",
            "319 -> 2.0963711738586426 0.5633506774902344\n",
            "329 -> 2.123033285140991 0.5632023811340332\n",
            "339 -> 2.0757598876953125 0.5647540092468262\n",
            "349 -> 2.0553808212280273 0.5645601749420166\n",
            "359 -> 2.0653998851776123 0.5642955303192139\n",
            "369 -> 2.126188278198242 0.5626561641693115\n",
            "379 -> 2.042811870574951 0.5638189315795898\n",
            "389 -> 2.0795328617095947 0.5641000270843506\n",
            "399 -> 2.057553291320801 0.5629913806915283\n",
            "409 -> 2.0346620082855225 0.563971996307373\n",
            "419 -> 2.0485973358154297 0.5637927055358887\n",
            "429 -> 2.0375094413757324 0.5649478435516357\n",
            "439 -> 2.0419979095458984 0.5632514953613281\n",
            "449 -> 2.017897605895996 0.5646646022796631\n",
            "459 -> 1.9973974227905273 0.563504695892334\n",
            "469 -> 2.0387353897094727 0.5623834133148193\n",
            "479 -> 2.0380654335021973 0.564403772354126\n",
            "489 -> 2.0244765281677246 0.5636117458343506\n",
            "499 -> 1.9860789775848389 0.5642569065093994\n",
            "509 -> 1.995619297027588 0.5628006458282471\n",
            "519 -> 1.974392294883728 0.5650458335876465\n",
            "529 -> 2.0428340435028076 0.5622177124023438\n",
            "539 -> 2.0026841163635254 0.5641064643859863\n",
            "549 -> 1.9668145179748535 0.5633487701416016\n",
            "559 -> 2.0232017040252686 0.5637176036834717\n",
            "569 -> 2.0372531414031982 0.5647389888763428\n",
            "579 -> 1.9779274463653564 0.5644204616546631\n",
            "589 -> 1.9830716848373413 0.5628044605255127\n",
            "599 -> 2.0049028396606445 0.5644497871398926\n",
            "609 -> 2.0037927627563477 0.5633838176727295\n",
            "619 -> 1.978273630142212 0.5644586086273193\n",
            "629 -> 2.0152974128723145 0.563037633895874\n",
            "639 -> 2.044219970703125 0.565173864364624\n",
            "649 -> 1.9716790914535522 0.5632927417755127\n",
            "659 -> 2.0416083335876465 0.5646762847900391\n",
            "669 -> 2.0475544929504395 0.5640010833740234\n",
            "679 -> 1.986319899559021 0.5631201267242432\n",
            "689 -> 1.9886664152145386 0.5636258125305176\n",
            "699 -> 2.0099105834960938 0.5660724639892578\n",
            "709 -> 1.9866141080856323 0.5645380020141602\n",
            "719 -> 1.9988316297531128 0.5621757507324219\n",
            "729 -> 1.9772180318832397 0.5653269290924072\n",
            "739 -> 2.0013809204101562 0.5631744861602783\n",
            "749 -> 1.9769500494003296 0.5632591247558594\n",
            "759 -> 1.994429588317871 0.5630180835723877\n",
            "769 -> 1.9306882619857788 0.5639970302581787\n",
            "779 -> 1.9226694107055664 0.5640206336975098\n",
            "789 -> 1.9643447399139404 0.5645802021026611\n",
            "799 -> 1.9425792694091797 0.5638809204101562\n",
            "809 -> 1.9642295837402344 0.5641863346099854\n",
            "819 -> 1.9216445684432983 0.564561128616333\n",
            "829 -> 1.937727689743042 0.5625998973846436\n",
            "839 -> 1.9937270879745483 0.5653223991394043\n",
            "849 -> 1.9315052032470703 0.564279317855835\n",
            "859 -> 1.9214503765106201 0.5625400543212891\n",
            "869 -> 1.9164012670516968 0.5641896724700928\n",
            "879 -> 2.012227773666382 0.5641293525695801\n",
            "889 -> 1.9481065273284912 0.5634326934814453\n",
            "899 -> 1.9606598615646362 0.5624043941497803\n",
            "909 -> 1.9619983434677124 0.5646755695343018\n",
            "919 -> 1.9917958974838257 0.5636708736419678\n",
            "929 -> 2.005117177963257 0.5673987865447998\n",
            "939 -> 1.9588388204574585 0.563316822052002\n",
            "949 -> 1.9699811935424805 0.5634677410125732\n",
            "959 -> 1.937360167503357 0.5637857913970947\n",
            "969 -> 1.965200424194336 0.5641601085662842\n",
            "979 -> 1.949188232421875 0.5638165473937988\n",
            "989 -> 1.8992122411727905 0.5639867782592773\n",
            "999 -> 1.9760448932647705 0.5628485679626465\n"
          ]
        }
      ],
      "source": [
        "BATCH_IN_SEQUENCES = 384\n",
        "SEQUENCE_LENGTH = 128\n",
        "\n",
        "VOCAB_DIM = 256\n",
        "EMBED_DIM = 512\n",
        "FF_DIM = 2048\n",
        "\n",
        "NUM_HEADS = 4\n",
        "HEAD_DIM = 128\n",
        "\n",
        "LAYERS = 2\n",
        "\n",
        "HEAD_DEPTH = 128\n",
        "\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "FSDP = 4\n",
        "TENSOR = 1\n",
        "\n",
        "\n",
        "def attention_ourselves(_Q, _K, _V):\n",
        "    _weights_unnormalized = jax.numpy.einsum(\"BSHD,BTHD->BHST\", _Q, _K)\n",
        "    _weights_unnormalized_to_zero_out = jax.numpy.triu( jax.numpy.ones((SEQUENCE_LENGTH,SEQUENCE_LENGTH), jax.numpy.bfloat16), 1)\n",
        "    _weights = jax.nn.softmax(_weights_unnormalized - 1e6 * _weights_unnormalized_to_zero_out)  ### Creating something of size (B,HEADS, SEQUENCE, SEQUENCE)\n",
        "    #print(f\"{_weights.size=}\")\n",
        "    output = jax.numpy.einsum(\"BHST,BTHD->BSHD\", _weights, _V)\n",
        "\n",
        "    return output\n",
        "\n",
        "class OurModel(nn.Module):\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    '''\n",
        "        x is [BATCH, SEQUENCE]\n",
        "    '''\n",
        "    embedding = self.param(\n",
        "        'embedding',\n",
        "        nn.with_partitioning(nn.initializers.normal(1), (\"tp\", \"fsdp\")),\n",
        "        (VOCAB_DIM, EMBED_DIM),\n",
        "        jnp.float32,\n",
        "    )\n",
        "    x = embedding[x] ##OUTPUT should be [BATCH, SEQUENCE, EMBED]\n",
        "\n",
        "    # positional_embedding = self.param(\n",
        "    #     'positional_embedding',\n",
        "    #     nn.with_partitioning(nn.initializers.normal(1), (None, None, \"fsdp\")),\n",
        "    #     (1, SEQUENCE_LENGTH, EMBED_DIM),\n",
        "    #     jnp.float32,\n",
        "    # )\n",
        "\n",
        "    # x += positional_embedding\n",
        "\n",
        "\n",
        "    for i in range(LAYERS):\n",
        "      feedforward = self.param(\n",
        "          'feedforward_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('fsdp', 'tp')),\n",
        "          (EMBED_DIM, FF_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      x = x @ feedforward\n",
        "      x = jax.nn.relu(x)\n",
        "      embed = self.param(\n",
        "          'embed_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('tp', 'fsdp')),\n",
        "          (FF_DIM, EMBED_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      x = x @ embed\n",
        "      x = jax.nn.relu(x)\n",
        "\n",
        "      q_proj = self.param(\n",
        "          'qproj_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('fsdp', 'tp')),\n",
        "          (EMBED_DIM, NUM_HEADS, HEAD_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      q = jnp.einsum(\"BSE,EHD->BSHD\",x, q_proj )\n",
        "\n",
        "      k_proj = self.param(\n",
        "          'kproj_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('fsdp', 'tp')),\n",
        "          (EMBED_DIM, NUM_HEADS, HEAD_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      k = jnp.einsum(\"BSE,EHD->BSHD\",x, k_proj )\n",
        "\n",
        "      v_proj = self.param(\n",
        "          'vproj_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('fsdp', 'tp')),\n",
        "          (EMBED_DIM, NUM_HEADS, HEAD_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      v = jnp.einsum(\"BSE,EHD->BSHD\",x, v_proj )\n",
        "\n",
        "      o = attention_ourselves(q,k,v)\n",
        "\n",
        "      o_proj = self.param(\n",
        "          'oproj_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('fsdp', 'tp')),\n",
        "          (NUM_HEADS, HEAD_DIM, EMBED_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      x = jnp.einsum(\"BSHD,HDE->BSE\",o, o_proj )\n",
        "\n",
        "    return x @ embedding.T # missing the softmax with probabilities for each token?\n",
        "\n",
        "def convert_to_ascii(string_array, max_length):\n",
        "  result = np.zeros((len(string_array), max_length), dtype=np.uint8)\n",
        "  for i, string in enumerate(string_array):\n",
        "    for j, char in enumerate(string):\n",
        "      if j >= SEQUENCE_LENGTH:\n",
        "         break\n",
        "      result[i, j] = char\n",
        "  return result\n",
        "\n",
        "def input_to_output(np_array):\n",
        "   zero_array = np.zeros( (BATCH_IN_SEQUENCES,SEQUENCE_LENGTH), dtype = jnp.uint8)\n",
        "   zero_array[:, 1:SEQUENCE_LENGTH] = np_array[:, 0:SEQUENCE_LENGTH-1]\n",
        "   return zero_array\n",
        "\n",
        "def calculate_loss(params, model, inputs, outputs):\n",
        "   proposed_outputs = model.apply(params, inputs)\n",
        "   one_hot = jax.nn.one_hot(outputs, VOCAB_DIM)\n",
        "   loss = optax.softmax_cross_entropy(proposed_outputs, one_hot)\n",
        "   return jnp.mean(loss)\n",
        "\n",
        "\n",
        "def step(state, model, inputs, outputs):\n",
        "   loss, grad = jax.value_and_grad(calculate_loss)(state.params, model, inputs, outputs)\n",
        "   state = state.apply_gradients(grads = grad)\n",
        "   return loss, state\n",
        "\n",
        "\n",
        "mesh = jax.sharding.Mesh(np.reshape(  jax.devices(), (FSDP,TENSOR)), [\"fsdp\", \"tp\"])\n",
        "\n",
        "ds = tfds.load('lm1b', split='train', shuffle_files=False)\n",
        "ds = ds.batch(BATCH_IN_SEQUENCES)\n",
        "\n",
        "rngkey = jax.random.key(0)\n",
        "model = OurModel()\n",
        "\n",
        "shaped_init = jax.eval_shape( functools.partial(model.init, rngkey), jax.ShapeDtypeStruct((BATCH_IN_SEQUENCES, SEQUENCE_LENGTH), dtype = jnp.uint8))\n",
        "state_sharding = nn.get_sharding(shaped_init, mesh)\n",
        "_params = jax.jit(model.init, out_shardings = state_sharding)(rngkey, jax.ShapeDtypeStruct((BATCH_IN_SEQUENCES, SEQUENCE_LENGTH), dtype = jnp.uint8))\n",
        "\n",
        "tx = optax.adam(learning_rate = LEARNING_RATE)\n",
        "state = train_state.TrainState.create(\n",
        "    apply_fn = model.apply,\n",
        "    params = _params,\n",
        "    tx = tx\n",
        ")\n",
        "\n",
        "iter = 0\n",
        "static_step = jax.jit(step, static_argnums=1)\n",
        "\n",
        "last_step_time = time.time()\n",
        "stepnum = 0\n",
        "\n",
        "for example in ds:\n",
        "    outputs = convert_to_ascii(example['text'].numpy(), SEQUENCE_LENGTH)\n",
        "    inputs = input_to_output(outputs)\n",
        "\n",
        "    loss, state = static_step(state, model, inputs, outputs)\n",
        "    # assert model_without_pos_embeddings(\"Sam and Erin\") == model_without_pos_embeddings(\"Erin and Sam\")\n",
        "    # assert model_with_pos_embeddings(\"Sam and Erin\") != model_with_pos_embeddings(\"Erin and Sam\")\n",
        "    #loss, state = jax.jit(step, static_argnums=1)(state, model, inputs, outputs)\n",
        "    #loss, state = jax.jit(lambda x,y,z,a : step(x,y,z,a), static_argnums=1)(state, model, inputs, outputs)\n",
        "\n",
        "    stepnum += 1\n",
        "\n",
        "    if stepnum % 10 == 0:\n",
        "      new_time = time.time()\n",
        "      time_elapsed_seconds = (new_time-last_step_time)\n",
        "      last_step_time = new_time\n",
        "      print(f\"{iter} -> {loss} {time_elapsed_seconds}\")\n",
        "\n",
        "\n",
        "    iter += 1\n",
        "    if stepnum == 1000:\n",
        "      break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 128, 256)\n",
            "logits 1 and 2 are the same: True\n",
            "logits 1 and 3 are the same: False\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def predict(input_str, model, params):\n",
        "    input_tf = tf.constant([input_str], shape=(1,), dtype=tf.string)\n",
        "    input_ascii = convert_to_ascii(input_tf.numpy(), SEQUENCE_LENGTH)\n",
        "    logits = model.apply(params, input_ascii)\n",
        "    return logits\n",
        "\n",
        "# for example in ds:\n",
        "#     print(example[\"text\"][1:2])\n",
        "#     inputs = convert_to_ascii(example['text'][1:2].numpy(), SEQUENCE_LENGTH)\n",
        "#     break\n",
        "logits_1 = predict(\"test a\", model, state.params)\n",
        "logits_2 = predict(\"test a\", model, state.params)\n",
        "logits_3 = predict(\"a test\", model, state.params)\n",
        "print(logits_1.shape)\n",
        "print(\"logits 1 and 2 are the same:\", jnp.array_equal(logits_1, logits_2))\n",
        "print(\"logits 1 and 3 are the same:\", jnp.array_equal(logits_1, logits_3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logits shape: (1, 128, 256)\n",
            "logits 1 and 2 are the same: False\n",
            "logits 2 and 3 are the same: True\n",
            "logits_1[0][0][0]=Array(8.40613, dtype=float32)\n",
            "logits_2[0][0][0]=Array(1.3883572, dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "# Near the end of your script, replace the definition of input_1\n",
        "\n",
        "def pad(raw_input_1):\n",
        "    padding_length = SEQUENCE_LENGTH - raw_input_1.shape[1]\n",
        "    if padding_length < 0:\n",
        "        input_1 = raw_input_1[:, :SEQUENCE_LENGTH]\n",
        "    elif padding_length > 0:\n",
        "        input_1 = jnp.pad(raw_input_1, ((0, 0), (0, padding_length)), mode='constant', constant_values=0)\n",
        "    else:\n",
        "        input_1 = raw_input_1\n",
        "    return input_1\n",
        "\n",
        "\n",
        "raw_input_1 = jnp.array([[10, 5]], dtype=jnp.int32)\n",
        "raw_input_2 = jnp.array([[5, 10]], dtype=jnp.int32)\n",
        "input_1 = pad(raw_input_1)\n",
        "input_2 = pad(raw_input_2)\n",
        "\n",
        "def predict_tensor(ascii_tensor, model, params):\n",
        "    logits = model.apply(params, ascii_tensor)\n",
        "    return logits\n",
        "\n",
        "logits_1 = predict_tensor(input_1, model, state.params) # Use the padded input\n",
        "logits_2 = predict_tensor(input_2, model, state.params) # Use the padded input\n",
        "logits_3 = predict_tensor(input_2, model, state.params) # Use the padded input\n",
        "print(\"Logits shape:\", logits_1.shape)\n",
        "print(\"logits 1 and 2 are the same:\", jnp.array_equal(logits_1, logits_2))\n",
        "print(\"logits 2 and 3 are the same:\", jnp.array_equal(logits_2, logits_3))\n",
        "print(f\"{logits_1[0][0][0]=}\")\n",
        "print(f\"{logits_2[0][0][0]=}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
