{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFeyr3-g9n2t",
        "outputId": "42dade45-1810-456c-e803-8f2acb03c4ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting flax\n",
            "  Downloading flax-0.10.5-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting tensorflow_datasets\n",
            "  Downloading tensorflow_datasets-4.9.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting jax>=0.5.1 (from flax)\n",
            "  Downloading jax-0.5.3-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting msgpack (from flax)\n",
            "  Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
            "Collecting optax (from flax)\n",
            "  Downloading optax-0.2.4-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting orbax-checkpoint (from flax)\n",
            "  Downloading orbax_checkpoint-0.11.12-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting tensorstore (from flax)\n",
            "  Downloading tensorstore-0.1.73-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Collecting rich>=11.1 (from flax)\n",
            "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.2 in ./.venv/lib/python3.10/site-packages (from flax) (4.12.2)\n",
            "Collecting PyYAML>=5.4.1 (from flax)\n",
            "  Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting treescope>=0.1.7 (from flax)\n",
            "  Downloading treescope-0.1.9-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting absl-py>=1.0.0 (from tensorflow)\n",
            "  Downloading absl_py-2.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
            "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
            "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
            "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: packaging in ./.venv/lib/python3.10/site-packages (from tensorflow) (24.2)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)\n",
            "  Downloading protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Collecting requests<3,>=2.21.0 (from tensorflow)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting setuptools (from tensorflow)\n",
            "  Downloading setuptools-78.1.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: six>=1.12.0 in ./.venv/lib/python3.10/site-packages (from tensorflow) (1.17.0)\n",
            "Collecting termcolor>=1.1.0 (from tensorflow)\n",
            "  Downloading termcolor-3.0.1-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting wrapt>=1.11.0 (from tensorflow)\n",
            "  Downloading wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
            "  Downloading grpcio-1.71.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting keras>=3.5.0 (from tensorflow)\n",
            "  Downloading keras-3.9.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting numpy<2.2.0,>=1.26.0 (from tensorflow)\n",
            "  Downloading numpy-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Collecting h5py>=3.11.0 (from tensorflow)\n",
            "  Downloading h5py-3.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
            "  Downloading ml_dtypes-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Collecting array_record>=0.5.0 (from tensorflow_datasets)\n",
            "  Downloading array_record-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (877 bytes)\n",
            "Collecting dm-tree (from tensorflow_datasets)\n",
            "  Downloading dm_tree-0.1.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
            "Collecting etils>=1.6.0 (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow_datasets)\n",
            "  Downloading etils-1.12.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting immutabledict (from tensorflow_datasets)\n",
            "  Downloading immutabledict-4.2.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting promise (from tensorflow_datasets)\n",
            "  Downloading promise-2.3.tar.gz (19 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: psutil in ./.venv/lib/python3.10/site-packages (from tensorflow_datasets) (7.0.0)\n",
            "Collecting pyarrow (from tensorflow_datasets)\n",
            "  Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting simple_parsing (from tensorflow_datasets)\n",
            "  Downloading simple_parsing-0.1.7-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting tensorflow-metadata (from tensorflow_datasets)\n",
            "  Downloading tensorflow_metadata-1.17.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting toml (from tensorflow_datasets)\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting tqdm (from tensorflow_datasets)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting einops (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow_datasets)\n",
            "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting fsspec (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow_datasets)\n",
            "  Downloading fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting importlib_resources (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow_datasets)\n",
            "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: zipp in ./.venv/lib/python3.10/site-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow_datasets) (3.20.2)\n",
            "Collecting jaxlib<=0.5.3,>=0.5.3 (from jax>=0.5.1->flax)\n",
            "  Downloading jaxlib-0.5.3-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting scipy>=1.11.1 (from jax>=0.5.1->flax)\n",
            "  Downloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting namex (from keras>=3.5.0->tensorflow)\n",
            "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
            "Collecting optree (from keras>=3.5.0->tensorflow)\n",
            "  Downloading optree-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.21.0->tensorflow)\n",
            "  Downloading charset_normalizer-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
            "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorflow)\n",
            "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorflow)\n",
            "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorflow)\n",
            "  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich>=11.1->flax)\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.10/site-packages (from rich>=11.1->flax) (2.19.1)\n",
            "Collecting markdown>=2.6.8 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading markdown-3.8-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: attrs>=18.2.0 in ./.venv/lib/python3.10/site-packages (from dm-tree->tensorflow_datasets) (24.2.0)\n",
            "Collecting chex>=0.1.87 (from optax->flax)\n",
            "  Downloading chex-0.1.89-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: nest_asyncio in ./.venv/lib/python3.10/site-packages (from orbax-checkpoint->flax) (1.6.0)\n",
            "Collecting humanize (from orbax-checkpoint->flax)\n",
            "  Downloading humanize-4.12.2-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting simplejson>=3.16.0 (from orbax-checkpoint->flax)\n",
            "  Downloading simplejson-3.20.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Collecting docstring-parser<1.0,>=0.15 (from simple_parsing->tensorflow_datasets)\n",
            "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)\n",
            "  Downloading protobuf-4.21.12-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting toolz>=0.9.0 (from chex>=0.1.87->optax->flax)\n",
            "  Downloading toolz-1.0.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1->flax)\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Downloading flax-0.10.5-py3-none-any.whl (456 kB)\n",
            "Downloading tensorflow-2.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.8/644.8 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_datasets-4.9.8-py3-none-any.whl (5.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m129.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading absl_py-2.2.2-py3-none-any.whl (135 kB)\n",
            "Downloading array_record-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading etils-1.12.2-py3-none-any.whl (167 kB)\n",
            "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "Downloading grpcio-1.71.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m139.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h5py-3.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m123.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax-0.5.3-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m124.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-3.9.2-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
            "Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m751.2/751.2 kB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
            "Downloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached setuptools-78.1.0-py3-none-any.whl (1.3 MB)\n",
            "Downloading tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading termcolor-3.0.1-py3-none-any.whl (7.2 kB)\n",
            "Downloading treescope-0.1.9-py3-none-any.whl (182 kB)\n",
            "Downloading wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (82 kB)\n",
            "Downloading dm_tree-0.1.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB)\n",
            "Downloading immutabledict-4.2.1-py3-none-any.whl (4.7 kB)\n",
            "Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (378 kB)\n",
            "Downloading optax-0.2.4-py3-none-any.whl (319 kB)\n",
            "Downloading orbax_checkpoint-0.11.12-py3-none-any.whl (406 kB)\n",
            "Downloading tensorstore-0.1.73-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading simple_parsing-0.1.7-py3-none-any.whl (112 kB)\n",
            "Downloading tensorflow_metadata-1.17.1-py3-none-any.whl (31 kB)\n",
            "Downloading protobuf-4.21.12-cp37-abi3-manylinux2014_x86_64.whl (409 kB)\n",
            "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
            "Downloading charset_normalizer-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (146 kB)\n",
            "Downloading chex-0.1.89-py3-none-any.whl (99 kB)\n",
            "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
            "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "Downloading jaxlib-0.5.3-cp310-cp310-manylinux2014_x86_64.whl (105.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading markdown-3.8-py3-none-any.whl (106 kB)\n",
            "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "Downloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading simplejson-3.20.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
            "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "Downloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
            "Downloading fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
            "Downloading humanize-4.12.2-py3-none-any.whl (128 kB)\n",
            "Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
            "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Downloading optree-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (397 kB)\n",
            "Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
            "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Downloading toolz-1.0.0-py3-none-any.whl (56 kB)\n",
            "Building wheels for collected packages: promise\n",
            "  Building wheel for promise (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21581 sha256=c4fdb45dbdd1c880299a4786ed2b11a8d1587d5effa98ee418b41972daa223b1\n",
            "  Stored in directory: /home/stoelinga/.cache/pip/wheels/54/4e/28/3ed0e1c8a752867445bab994d2340724928aa3ab059c57c8db\n",
            "Successfully built promise\n",
            "Installing collected packages: namex, libclang, flatbuffers, wrapt, wheel, urllib3, tqdm, toolz, toml, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, simplejson, setuptools, PyYAML, pyarrow, protobuf, promise, optree, opt-einsum, numpy, msgpack, mdurl, MarkupSafe, markdown, importlib_resources, immutabledict, idna, humanize, grpcio, google-pasta, gast, fsspec, etils, einops, docstring-parser, charset-normalizer, certifi, absl-py, werkzeug, treescope, tensorflow-metadata, simple_parsing, scipy, requests, ml-dtypes, markdown-it-py, h5py, dm-tree, astunparse, tensorstore, tensorboard, rich, jaxlib, keras, jax, array_record, tensorflow, orbax-checkpoint, chex, tensorflow_datasets, optax, flax\n",
            "Successfully installed MarkupSafe-3.0.2 PyYAML-6.0.2 absl-py-2.2.2 array_record-0.7.1 astunparse-1.6.3 certifi-2025.1.31 charset-normalizer-3.4.1 chex-0.1.89 dm-tree-0.1.9 docstring-parser-0.16 einops-0.8.1 etils-1.12.2 flatbuffers-25.2.10 flax-0.10.5 fsspec-2025.3.2 gast-0.6.0 google-pasta-0.2.0 grpcio-1.71.0 h5py-3.13.0 humanize-4.12.2 idna-3.10 immutabledict-4.2.1 importlib_resources-6.5.2 jax-0.5.3 jaxlib-0.5.3 keras-3.9.2 libclang-18.1.1 markdown-3.8 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.5.1 msgpack-1.1.0 namex-0.0.8 numpy-2.1.3 opt-einsum-3.4.0 optax-0.2.4 optree-0.15.0 orbax-checkpoint-0.11.12 promise-2.3 protobuf-4.21.12 pyarrow-19.0.1 requests-2.32.3 rich-14.0.0 scipy-1.15.2 setuptools-78.1.0 simple_parsing-0.1.7 simplejson-3.20.1 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.37.1 tensorflow-metadata-1.17.1 tensorflow_datasets-4.9.8 tensorstore-0.1.73 termcolor-3.0.1 toml-0.10.2 toolz-1.0.0 tqdm-4.67.1 treescope-0.1.9 urllib3-2.4.0 werkzeug-3.1.3 wheel-0.45.1 wrapt-1.17.2\n"
          ]
        }
      ],
      "source": [
        "! pip install flax tensorflow tensorflow_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jax[tpu] in ./.venv/lib/python3.10/site-packages (0.5.3)\n",
            "Requirement already satisfied: jaxlib<=0.5.3,>=0.5.3 in ./.venv/lib/python3.10/site-packages (from jax[tpu]) (0.5.3)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in ./.venv/lib/python3.10/site-packages (from jax[tpu]) (0.5.1)\n",
            "Requirement already satisfied: numpy>=1.25 in ./.venv/lib/python3.10/site-packages (from jax[tpu]) (2.1.3)\n",
            "Requirement already satisfied: opt_einsum in ./.venv/lib/python3.10/site-packages (from jax[tpu]) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in ./.venv/lib/python3.10/site-packages (from jax[tpu]) (1.15.2)\n",
            "Collecting libtpu==0.0.11.* (from jax[tpu])\n",
            "  Downloading libtpu-0.0.11.1-py3-none-manylinux_2_31_x86_64.whl.metadata (556 bytes)\n",
            "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from jax[tpu]) (2.32.3)\n",
            "Collecting tpu-info>=0.2.0 (from libtpu==0.0.11.*->jax[tpu])\n",
            "  Downloading tpu_info-0.2.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->jax[tpu]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests->jax[tpu]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->jax[tpu]) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->jax[tpu]) (2025.1.31)\n",
            "Requirement already satisfied: grpcio>=1.65.5 in ./.venv/lib/python3.10/site-packages (from tpu-info>=0.2.0->libtpu==0.0.11.*->jax[tpu]) (1.71.0)\n",
            "Requirement already satisfied: protobuf in ./.venv/lib/python3.10/site-packages (from tpu-info>=0.2.0->libtpu==0.0.11.*->jax[tpu]) (4.21.12)\n",
            "Requirement already satisfied: rich in ./.venv/lib/python3.10/site-packages (from tpu-info>=0.2.0->libtpu==0.0.11.*->jax[tpu]) (14.0.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.10/site-packages (from rich->tpu-info>=0.2.0->libtpu==0.0.11.*->jax[tpu]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.10/site-packages (from rich->tpu-info>=0.2.0->libtpu==0.0.11.*->jax[tpu]) (2.19.1)\n",
            "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in ./.venv/lib/python3.10/site-packages (from rich->tpu-info>=0.2.0->libtpu==0.0.11.*->jax[tpu]) (4.12.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->tpu-info>=0.2.0->libtpu==0.0.11.*->jax[tpu]) (0.1.2)\n",
            "Downloading libtpu-0.0.11.1-py3-none-manylinux_2_31_x86_64.whl (130.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.4/130.4 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading tpu_info-0.2.0-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: tpu-info, libtpu\n",
            "Successfully installed libtpu-0.0.11.1 tpu-info-0.2.0\n"
          ]
        }
      ],
      "source": [
        "! pip install \"jax[tpu]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
              " TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
              " TpuDevice(id=2, process_index=0, coords=(2,0,0), core_on_chip=0),\n",
              " TpuDevice(id=3, process_index=0, coords=(3,0,0), core_on_chip=0),\n",
              " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
              " TpuDevice(id=5, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
              " TpuDevice(id=6, process_index=0, coords=(2,1,0), core_on_chip=0),\n",
              " TpuDevice(id=7, process_index=0, coords=(3,1,0), core_on_chip=0)]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import jax\n",
        "jax.devices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1Tu1Cjb-83aw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-12 01:54:40.969001: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-04-12 01:54:40.972096: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-04-12 01:54:40.982209: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744422880.999972   25942 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744422881.005215   25942 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1744422881.017742   25942 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1744422881.017755   25942 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1744422881.017758   25942 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1744422881.017759   25942 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-04-12 01:54:41.021769: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/home/stoelinga/.venv/lib/python3.10/site-packages/jax/_src/cloud_tpu_init.py:82: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.linen as nn\n",
        "from flax.training import train_state\n",
        "\n",
        "import functools\n",
        "\n",
        "import flax.linen.attention as attention\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import optax\n",
        "\n",
        "import time\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "Nve1Jxq_9bBD",
        "outputId": "04be3b44-6df1-44c0-92d5-102de758df0b"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "cannot reshape array of size 8 into shape (4,1)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 136\u001b[0m\n\u001b[1;32m    132\u001b[0m    state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mapply_gradients(grads \u001b[38;5;241m=\u001b[39m grad)\n\u001b[1;32m    133\u001b[0m    \u001b[38;5;28;01mreturn\u001b[39;00m loss, state\n\u001b[0;32m--> 136\u001b[0m mesh \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39msharding\u001b[38;5;241m.\u001b[39mMesh(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mFSDP\u001b[49m\u001b[43m,\u001b[49m\u001b[43mTENSOR\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfsdp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtp\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    138\u001b[0m ds \u001b[38;5;241m=\u001b[39m tfds\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlm1b\u001b[39m\u001b[38;5;124m'\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, shuffle_files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    139\u001b[0m ds \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mbatch(BATCH_IN_SEQUENCES)\n",
            "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:328\u001b[0m, in \u001b[0;36mreshape\u001b[0;34m(a, shape, order, newshape, copy)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreshape\u001b[39m\u001b[38;5;124m'\u001b[39m, shape, order\u001b[38;5;241m=\u001b[39morder, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m--> 328\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreshape\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:54\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     52\u001b[0m bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, method, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bound \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
            "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:46\u001b[0m, in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# As this already tried the method, subok is maybe quite reasonable here\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# but this follows what was done before. TODO: revisit this.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m arr, \u001b[38;5;241m=\u001b[39m conv\u001b[38;5;241m.\u001b[39mas_arrays(subok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 46\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conv\u001b[38;5;241m.\u001b[39mwrap(result, to_scalar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 8 into shape (4,1)"
          ]
        }
      ],
      "source": [
        "BATCH_IN_SEQUENCES = 384\n",
        "SEQUENCE_LENGTH = 128\n",
        "\n",
        "VOCAB_DIM = 256\n",
        "EMBED_DIM = 512\n",
        "FF_DIM = 2048\n",
        "\n",
        "NUM_HEADS = 4\n",
        "HEAD_DIM = 128\n",
        "\n",
        "LAYERS = 2\n",
        "\n",
        "HEAD_DEPTH = 128\n",
        "NUM_HEADS = 4\n",
        "\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "FSDP = 4 # 8 OR 4\n",
        "TENSOR = 1 # 1 OR 2\n",
        "\n",
        "\n",
        "def attention_ourselves(_Q, _K, _V):\n",
        "    _weights_unnormalized = jax.numpy.einsum(\"BSHD,BTHD->BHST\", _Q, _K)\n",
        "    _weights_unnormalized_to_zero_out = jax.numpy.triu( jax.numpy.ones((SEQUENCE_LENGTH,SEQUENCE_LENGTH), jax.numpy.bfloat16), 1)\n",
        "    _weights = jax.nn.softmax(_weights_unnormalized - 1e6 * _weights_unnormalized_to_zero_out)  ### Creating something of size (B,HEADS, SEQUENCE, SEQUENCE)\n",
        "    #print(f\"{_weights.size=}\")\n",
        "    output = jax.numpy.einsum(\"BHST,BTHD->BSHD\", _weights, _V)\n",
        "\n",
        "    return output\n",
        "\n",
        "class OurModel(nn.Module):\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    '''\n",
        "        x is [BATCH, SEQUENCE]\n",
        "    '''\n",
        "    embedding = self.param(\n",
        "        'embedding',\n",
        "        nn.with_partitioning(nn.initializers.normal(1), (\"tp\", \"fsdp\")),\n",
        "        (VOCAB_DIM, EMBED_DIM),\n",
        "        jnp.float32,\n",
        "    )\n",
        "    x = embedding[x] ##OUTPUT should be [BATCH, SEQUENCE, EMBED]\n",
        "\n",
        "    positional_embedding = self.param(\n",
        "        'positional_embedding',\n",
        "        nn.with_partitioning(nn.initializers.normal(1), (None, None, \"fsdp\")),\n",
        "        (1, SEQUENCE_LENGTH, EMBED_DIM),\n",
        "        jnp.float32,\n",
        "    )\n",
        "\n",
        "    x += positional_embedding\n",
        "\n",
        "\n",
        "    for i in range(LAYERS):\n",
        "      feedforward = self.param(\n",
        "          'feedforward_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('fsdp', 'tp')),\n",
        "          (EMBED_DIM, FF_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      x = x @ feedforward\n",
        "      x = jax.nn.relu(x)\n",
        "      embed = self.param(\n",
        "          'embed_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('tp', 'fsdp')),\n",
        "          (FF_DIM, EMBED_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      x = x @ embed\n",
        "      x = jax.nn.relu(x)\n",
        "\n",
        "      q_proj = self.param(\n",
        "          'qproj_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('fsdp', 'tp')),\n",
        "          (EMBED_DIM, NUM_HEADS, HEAD_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      q = jnp.einsum(\"BSE,EHD->BSHD\",x, q_proj )\n",
        "\n",
        "      k_proj = self.param(\n",
        "          'kproj_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('fsdp', 'tp')),\n",
        "          (EMBED_DIM, NUM_HEADS, HEAD_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      k = jnp.einsum(\"BSE,EHD->BSHD\",x, k_proj )\n",
        "\n",
        "      v_proj = self.param(\n",
        "          'vproj_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('fsdp', 'tp')),\n",
        "          (EMBED_DIM, NUM_HEADS, HEAD_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      v = jnp.einsum(\"BSE,EHD->BSHD\",x, v_proj )\n",
        "\n",
        "      o = attention_ourselves(q,k,v)\n",
        "\n",
        "      o_proj = self.param(\n",
        "          'oproj_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('fsdp', 'tp')),\n",
        "          (NUM_HEADS, HEAD_DIM, EMBED_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      x = jnp.einsum(\"BSHD,HDE->BSE\",o, o_proj )\n",
        "\n",
        "    return x @ embedding.T\n",
        "\n",
        "def convert_to_ascii(string_array, max_length):\n",
        "  result = np.zeros((len(string_array), max_length), dtype=np.uint8)\n",
        "  for i, string in enumerate(string_array):\n",
        "    for j, char in enumerate(string):\n",
        "      if j >= SEQUENCE_LENGTH:\n",
        "         break\n",
        "      result[i, j] = char\n",
        "  return result\n",
        "\n",
        "def input_to_output(np_array):\n",
        "   zero_array = np.zeros( (BATCH_IN_SEQUENCES,SEQUENCE_LENGTH), dtype = jnp.uint8)\n",
        "   zero_array[:, 1:SEQUENCE_LENGTH] = np_array[:, 0:SEQUENCE_LENGTH-1]\n",
        "   return zero_array\n",
        "\n",
        "def calculate_loss(params, model, inputs, outputs):\n",
        "   proposed_outputs = model.apply(params, inputs)\n",
        "   one_hot = jax.nn.one_hot(outputs, VOCAB_DIM)\n",
        "   loss = optax.softmax_cross_entropy(proposed_outputs, one_hot)\n",
        "   return jnp.mean(loss)\n",
        "\n",
        "\n",
        "def step(state, model, inputs, outputs):\n",
        "   loss, grad = jax.value_and_grad(calculate_loss)(state.params, model, inputs, outputs)\n",
        "   state = state.apply_gradients(grads = grad)\n",
        "   return loss, state\n",
        "\n",
        "\n",
        "mesh = jax.sharding.Mesh(np.reshape(  jax.devices(), (FSDP,TENSOR)), [\"fsdp\", \"tp\"])\n",
        "\n",
        "ds = tfds.load('lm1b', split='train', shuffle_files=False)\n",
        "ds = ds.batch(BATCH_IN_SEQUENCES)\n",
        "\n",
        "rngkey = jax.random.key(0)\n",
        "model = OurModel()\n",
        "\n",
        "shaped_init = jax.eval_shape( functools.partial(model.init, rngkey), jax.ShapeDtypeStruct((BATCH_IN_SEQUENCES, SEQUENCE_LENGTH), dtype = jnp.uint8))\n",
        "state_sharding = nn.get_sharding(shaped_init, mesh)\n",
        "_params = jax.jit(model.init, out_shardings = state_sharding)(rngkey, jax.ShapeDtypeStruct((BATCH_IN_SEQUENCES, SEQUENCE_LENGTH), dtype = jnp.uint8))\n",
        "\n",
        "tx = optax.adam(learning_rate = LEARNING_RATE)\n",
        "state = train_state.TrainState.create(\n",
        "    apply_fn = model.apply,\n",
        "    params = _params,\n",
        "    tx = tx\n",
        ")\n",
        "\n",
        "iter = 0\n",
        "static_step = jax.jit(step, static_argnums=1)\n",
        "\n",
        "last_step_time = time.time()\n",
        "stepnum = 0\n",
        "\n",
        "for example in ds:\n",
        "    outputs = convert_to_ascii(example['text'].numpy(), SEQUENCE_LENGTH)\n",
        "    inputs = input_to_output(outputs)\n",
        "\n",
        "    loss, state = static_step(state, model, inputs, outputs)\n",
        "    #loss, state = jax.jit(step, static_argnums=1)(state, model, inputs, outputs)\n",
        "    #loss, state = jax.jit(lambda x,y,z,a : step(x,y,z,a), static_argnums=1)(state, model, inputs, outputs)\n",
        "\n",
        "    stepnum += 1\n",
        "\n",
        "    if stepnum % 10 == 0:\n",
        "      new_time = time.time()\n",
        "      time_elapsed_seconds = (new_time-last_step_time)\n",
        "      last_step_time = new_time\n",
        "      print(f\"{iter} -> {loss} {time_elapsed_seconds}\")\n",
        "\n",
        "\n",
        "    iter += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "eNhze1hYJqvo",
        "outputId": "154cf958-3ba3-43e7-cfb8-47a5da1ebf29"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "One of pjit outputs with pytree key path result['params']['oproj_0'].value was given the sharding of NamedSharding(mesh=Mesh('fsdp': 8, 'tp': 1, axis_types=(Auto, Auto)), spec=PartitionSpec('fsdp', 'tp'), memory_kind=device), which implies that the global size of its dimension 0 should be divisible by 8, but it is equal to 4 (full shape: (4, 128, 512))",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 145\u001b[0m\n\u001b[1;32m    143\u001b[0m shaped_init \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39meval_shape( functools\u001b[38;5;241m.\u001b[39mpartial(model\u001b[38;5;241m.\u001b[39minit, rngkey), jax\u001b[38;5;241m.\u001b[39mShapeDtypeStruct((BATCH_IN_SEQUENCES, SEQUENCE_LENGTH), dtype \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39muint8))\n\u001b[1;32m    144\u001b[0m state_sharding \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mget_sharding(shaped_init, mesh)\n\u001b[0;32m--> 145\u001b[0m _params \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_shardings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstate_sharding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrngkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mShapeDtypeStruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBATCH_IN_SEQUENCES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSEQUENCE_LENGTH\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m tx \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39madam(learning_rate \u001b[38;5;241m=\u001b[39m LEARNING_RATE)\n\u001b[1;32m    148\u001b[0m state \u001b[38;5;241m=\u001b[39m train_state\u001b[38;5;241m.\u001b[39mTrainState\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m    149\u001b[0m     apply_fn \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mapply,\n\u001b[1;32m    150\u001b[0m     params \u001b[38;5;241m=\u001b[39m _params,\n\u001b[1;32m    151\u001b[0m     tx \u001b[38;5;241m=\u001b[39m tx\n\u001b[1;32m    152\u001b[0m )\n",
            "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
            "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/jax/_src/pjit.py:1415\u001b[0m, in \u001b[0;36mpjit_check_aval_sharding\u001b[0;34m(shardings, flat_avals, names, what_aval, allow_uneven_sharding)\u001b[0m\n\u001b[1;32m   1413\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, size \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(num_ways_dim_sharded):\n\u001b[1;32m   1414\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_uneven_sharding \u001b[38;5;129;01mand\u001b[39;00m shape[i] \u001b[38;5;241m%\u001b[39m size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwhat_aval\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mname_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m was given the sharding \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1416\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, which implies that \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1417\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe global size of its dimension \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m should be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1418\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdivisible by \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but it is equal to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1419\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(full shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mValueError\u001b[0m: One of pjit outputs with pytree key path result['params']['oproj_0'].value was given the sharding of NamedSharding(mesh=Mesh('fsdp': 8, 'tp': 1, axis_types=(Auto, Auto)), spec=PartitionSpec('fsdp', 'tp'), memory_kind=device), which implies that the global size of its dimension 0 should be divisible by 8, but it is equal to 4 (full shape: (4, 128, 512))"
          ]
        }
      ],
      "source": [
        "BATCH_IN_SEQUENCES = 384\n",
        "SEQUENCE_LENGTH = 128\n",
        "\n",
        "VOCAB_DIM = 256\n",
        "EMBED_DIM = 512\n",
        "FF_DIM = 2048\n",
        "\n",
        "NUM_HEADS = 4\n",
        "HEAD_DIM = 128\n",
        "\n",
        "LAYERS = 2\n",
        "\n",
        "HEAD_DEPTH = 128\n",
        "\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "FSDP = 8\n",
        "TENSOR = 1\n",
        "\n",
        "\n",
        "def attention_ourselves(_Q, _K, _V):\n",
        "    _weights_unnormalized = jax.numpy.einsum(\"BSHD,BTHD->BHST\", _Q, _K)\n",
        "    _weights_unnormalized_to_zero_out = jax.numpy.triu( jax.numpy.ones((SEQUENCE_LENGTH,SEQUENCE_LENGTH), jax.numpy.bfloat16), 1)\n",
        "    _weights = jax.nn.softmax(_weights_unnormalized - 1e6 * _weights_unnormalized_to_zero_out)  ### Creating something of size (B,HEADS, SEQUENCE, SEQUENCE)\n",
        "    #print(f\"{_weights.size=}\")\n",
        "    output = jax.numpy.einsum(\"BHST,BTHD->BSHD\", _weights, _V)\n",
        "\n",
        "    return output\n",
        "\n",
        "class OurModel(nn.Module):\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    '''\n",
        "        x is [BATCH, SEQUENCE]\n",
        "    '''\n",
        "    embedding = self.param(\n",
        "        'embedding',\n",
        "        nn.with_partitioning(nn.initializers.normal(1), (\"tp\", \"fsdp\")),\n",
        "        (VOCAB_DIM, EMBED_DIM),\n",
        "        jnp.float32,\n",
        "    )\n",
        "    x = embedding[x] ##OUTPUT should be [BATCH, SEQUENCE, EMBED]\n",
        "\n",
        "    positional_embedding = self.param(\n",
        "        'positional_embedding',\n",
        "        nn.with_partitioning(nn.initializers.normal(1), (None, None, \"fsdp\")),\n",
        "        (1, SEQUENCE_LENGTH, EMBED_DIM),\n",
        "        jnp.float32,\n",
        "    )\n",
        "\n",
        "    x += positional_embedding\n",
        "\n",
        "\n",
        "    for i in range(LAYERS):\n",
        "      feedforward = self.param(\n",
        "          'feedforward_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('fsdp', 'tp')),\n",
        "          (EMBED_DIM, FF_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      x = x @ feedforward\n",
        "      x = jax.nn.relu(x)\n",
        "      embed = self.param(\n",
        "          'embed_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('tp', 'fsdp')),\n",
        "          (FF_DIM, EMBED_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      x = x @ embed\n",
        "      x = jax.nn.relu(x)\n",
        "\n",
        "      q_proj = self.param(\n",
        "          'qproj_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('fsdp', 'tp')),\n",
        "          (EMBED_DIM, NUM_HEADS, HEAD_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      q = jnp.einsum(\"BSE,EHD->BSHD\",x, q_proj )\n",
        "\n",
        "      k_proj = self.param(\n",
        "          'kproj_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('fsdp', 'tp')),\n",
        "          (EMBED_DIM, NUM_HEADS, HEAD_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      k = jnp.einsum(\"BSE,EHD->BSHD\",x, k_proj )\n",
        "\n",
        "      v_proj = self.param(\n",
        "          'vproj_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('fsdp', 'tp')),\n",
        "          (EMBED_DIM, NUM_HEADS, HEAD_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      v = jnp.einsum(\"BSE,EHD->BSHD\",x, v_proj )\n",
        "\n",
        "      o = attention_ourselves(q,k,v)\n",
        "\n",
        "      o_proj = self.param(\n",
        "          'oproj_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('fsdp', 'tp')),\n",
        "          (NUM_HEADS, HEAD_DIM, EMBED_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      x = jnp.einsum(\"BSHD,HDE->BSE\",o, o_proj )\n",
        "\n",
        "    return x @ embedding.T # missing the softmax with probabilities for each token?\n",
        "\n",
        "def convert_to_ascii(string_array, max_length):\n",
        "  result = np.zeros((len(string_array), max_length), dtype=np.uint8)\n",
        "  for i, string in enumerate(string_array):\n",
        "    for j, char in enumerate(string):\n",
        "      if j >= SEQUENCE_LENGTH:\n",
        "         break\n",
        "      result[i, j] = char\n",
        "  return result\n",
        "\n",
        "def input_to_output(np_array):\n",
        "   zero_array = np.zeros( (BATCH_IN_SEQUENCES,SEQUENCE_LENGTH), dtype = jnp.uint8)\n",
        "   zero_array[:, 1:SEQUENCE_LENGTH] = np_array[:, 0:SEQUENCE_LENGTH-1]\n",
        "   return zero_array\n",
        "\n",
        "def calculate_loss(params, model, inputs, outputs):\n",
        "   proposed_outputs = model.apply(params, inputs)\n",
        "   one_hot = jax.nn.one_hot(outputs, VOCAB_DIM)\n",
        "   loss = optax.softmax_cross_entropy(proposed_outputs, one_hot)\n",
        "   return jnp.mean(loss)\n",
        "\n",
        "\n",
        "def step(state, model, inputs, outputs):\n",
        "   loss, grad = jax.value_and_grad(calculate_loss)(state.params, model, inputs, outputs)\n",
        "   state = state.apply_gradients(grads = grad)\n",
        "   return loss, state\n",
        "\n",
        "\n",
        "mesh = jax.sharding.Mesh(np.reshape(  jax.devices(), (FSDP,TENSOR)), [\"fsdp\", \"tp\"])\n",
        "\n",
        "ds = tfds.load('lm1b', split='train', shuffle_files=False)\n",
        "ds = ds.batch(BATCH_IN_SEQUENCES)\n",
        "\n",
        "rngkey = jax.random.key(0)\n",
        "model = OurModel()\n",
        "\n",
        "shaped_init = jax.eval_shape( functools.partial(model.init, rngkey), jax.ShapeDtypeStruct((BATCH_IN_SEQUENCES, SEQUENCE_LENGTH), dtype = jnp.uint8))\n",
        "state_sharding = nn.get_sharding(shaped_init, mesh)\n",
        "_params = jax.jit(model.init, out_shardings = state_sharding)(rngkey, jax.ShapeDtypeStruct((BATCH_IN_SEQUENCES, SEQUENCE_LENGTH), dtype = jnp.uint8))\n",
        "\n",
        "tx = optax.adam(learning_rate = LEARNING_RATE)\n",
        "state = train_state.TrainState.create(\n",
        "    apply_fn = model.apply,\n",
        "    params = _params,\n",
        "    tx = tx\n",
        ")\n",
        "\n",
        "iter = 0\n",
        "static_step = jax.jit(step, static_argnums=1)\n",
        "\n",
        "last_step_time = time.time()\n",
        "stepnum = 0\n",
        "\n",
        "for example in ds:\n",
        "    outputs = convert_to_ascii(example['text'].numpy(), SEQUENCE_LENGTH)\n",
        "    inputs = input_to_output(outputs)\n",
        "\n",
        "    loss, state = static_step(state, model, inputs, outputs)\n",
        "    # assert model_without_pos_embeddings(\"Sam and Erin\") == model_without_pos_embeddings(\"Erin and Sam\")\n",
        "    # assert model_with_pos_embeddings(\"Sam and Erin\") != model_with_pos_embeddings(\"Erin and Sam\")\n",
        "    #loss, state = jax.jit(step, static_argnums=1)(state, model, inputs, outputs)\n",
        "    #loss, state = jax.jit(lambda x,y,z,a : step(x,y,z,a), static_argnums=1)(state, model, inputs, outputs)\n",
        "\n",
        "    stepnum += 1\n",
        "\n",
        "    if stepnum % 10 == 0:\n",
        "      new_time = time.time()\n",
        "      time_elapsed_seconds = (new_time-last_step_time)\n",
        "      last_step_time = new_time\n",
        "      print(f\"{iter} -> {loss} {time_elapsed_seconds}\")\n",
        "\n",
        "\n",
        "    iter += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Dsm88asD-LMA",
        "outputId": "ea17378b-3046-4879-becd-a36cdcd90ed5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-12 03:27:20.008887: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:387] The default buffer size is 262144, which is overridden by the user specified `buffer_size` of 8388608\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9 -> 4.784549713134766 8.760738372802734\n",
            "19 -> 3.8527989387512207 0.6371138095855713\n",
            "29 -> 3.095243215560913 0.6934900283813477\n",
            "39 -> 2.7191176414489746 0.6936655044555664\n",
            "49 -> 2.645170211791992 0.6936140060424805\n",
            "59 -> 2.5840907096862793 0.6946406364440918\n",
            "69 -> 2.620162010192871 0.6965792179107666\n",
            "79 -> 2.612311601638794 0.6946649551391602\n",
            "89 -> 2.6059045791625977 0.6969215869903564\n",
            "99 -> 2.552733898162842 0.6944339275360107\n",
            "109 -> 2.5300450325012207 0.6907339096069336\n",
            "119 -> 2.514218807220459 0.6982433795928955\n",
            "129 -> 2.5057194232940674 0.6927556991577148\n",
            "139 -> 2.566188097000122 0.6936328411102295\n",
            "149 -> 2.501035213470459 0.6959996223449707\n",
            "159 -> 2.4965708255767822 0.6950409412384033\n",
            "169 -> 2.5686490535736084 0.6938967704772949\n",
            "179 -> 2.6139049530029297 0.6937763690948486\n",
            "189 -> 2.6034724712371826 0.6953089237213135\n",
            "199 -> 2.512711524963379 0.6950550079345703\n",
            "209 -> 2.5235447883605957 0.6927204132080078\n",
            "219 -> 2.5821216106414795 0.6956758499145508\n",
            "229 -> 2.5074775218963623 0.6943163871765137\n",
            "239 -> 2.5503640174865723 0.6940040588378906\n",
            "249 -> 2.547011375427246 0.6999423503875732\n",
            "259 -> 2.529193878173828 0.6922883987426758\n",
            "269 -> 2.5340678691864014 0.6941416263580322\n",
            "279 -> 2.502808094024658 0.6937384605407715\n",
            "289 -> 3.2586333751678467 0.6959736347198486\n",
            "299 -> 2.6796531677246094 0.6951131820678711\n",
            "309 -> 2.628800868988037 0.6938967704772949\n",
            "319 -> 2.6122183799743652 0.6945521831512451\n",
            "329 -> 2.5519208908081055 0.6991250514984131\n",
            "339 -> 2.539741039276123 0.6889224052429199\n",
            "349 -> 2.5431039333343506 0.7027413845062256\n",
            "359 -> 2.572380781173706 0.6893813610076904\n",
            "369 -> 2.6470234394073486 0.6936008930206299\n",
            "379 -> 2.5572657585144043 0.6949863433837891\n",
            "389 -> 2.604410171508789 0.6940345764160156\n",
            "399 -> 2.5764100551605225 0.693007230758667\n",
            "409 -> 2.559469699859619 0.6949832439422607\n",
            "419 -> 2.556763172149658 0.6944198608398438\n",
            "429 -> 2.547504425048828 0.6942489147186279\n",
            "439 -> 2.5608205795288086 0.6933341026306152\n",
            "449 -> 2.539285659790039 0.6949772834777832\n",
            "459 -> 2.52047061920166 0.6932194232940674\n",
            "469 -> 2.572766065597534 0.6942477226257324\n",
            "479 -> 2.5790517330169678 0.6951816082000732\n",
            "489 -> 2.5630061626434326 0.69368577003479\n",
            "499 -> 2.513428211212158 0.6941947937011719\n",
            "509 -> 2.536557197570801 0.6937768459320068\n",
            "519 -> 2.5018537044525146 0.694312334060669\n",
            "529 -> 2.590402364730835 0.6940224170684814\n",
            "539 -> 2.5447943210601807 0.6942727565765381\n",
            "549 -> 2.4993131160736084 0.6940760612487793\n",
            "559 -> 2.576246500015259 0.6999120712280273\n",
            "569 -> 2.5877110958099365 0.6895112991333008\n",
            "579 -> 2.5248167514801025 0.6996679306030273\n",
            "589 -> 2.5254135131835938 0.6928257942199707\n",
            "599 -> 2.5562779903411865 0.6924097537994385\n",
            "609 -> 2.5681962966918945 0.6977419853210449\n",
            "619 -> 2.5236783027648926 0.6929495334625244\n",
            "629 -> 2.5702853202819824 0.6934363842010498\n",
            "639 -> 2.5716159343719482 0.6948354244232178\n",
            "649 -> 2.5278401374816895 0.6929223537445068\n",
            "659 -> 2.6147756576538086 0.6960773468017578\n",
            "669 -> 2.6357421875 0.6943283081054688\n",
            "679 -> 2.546382427215576 0.6926004886627197\n",
            "689 -> 2.5498197078704834 0.694587230682373\n",
            "699 -> 2.5690670013427734 0.6974539756774902\n",
            "709 -> 2.5464940071105957 0.6927201747894287\n",
            "719 -> 2.5727524757385254 0.6942746639251709\n",
            "729 -> 2.871493101119995 0.6942296028137207\n",
            "739 -> 3.4379701614379883 0.6947827339172363\n",
            "749 -> 2.894843816757202 0.6933996677398682\n",
            "759 -> 2.984652519226074 0.6942563056945801\n",
            "769 -> 2.6006972789764404 0.6947720050811768\n",
            "779 -> 2.709205389022827 0.6944096088409424\n",
            "789 -> 2.6469461917877197 0.697704553604126\n",
            "799 -> 2.6041207313537598 0.693218469619751\n",
            "809 -> 2.605797290802002 0.6961607933044434\n",
            "819 -> 2.5710577964782715 0.6987059116363525\n",
            "829 -> 2.551328420639038 0.6912667751312256\n",
            "839 -> 2.635420083999634 0.7006945610046387\n",
            "849 -> 2.5617332458496094 0.6894793510437012\n",
            "859 -> 2.543736457824707 0.6950314044952393\n",
            "869 -> 2.543802499771118 0.6922669410705566\n",
            "879 -> 2.6629350185394287 0.6979217529296875\n",
            "889 -> 2.5808441638946533 0.6943142414093018\n",
            "899 -> 2.599149465560913 0.6947503089904785\n",
            "909 -> 2.608196258544922 0.6968443393707275\n",
            "919 -> 2.6411590576171875 0.6919651031494141\n",
            "929 -> 2.6553244590759277 0.6955151557922363\n",
            "939 -> 2.5980701446533203 0.6952321529388428\n",
            "949 -> 2.611394166946411 0.6948513984680176\n",
            "959 -> 2.5714406967163086 0.693458080291748\n",
            "969 -> 2.610337972640991 0.6943678855895996\n",
            "979 -> 2.6092779636383057 0.6957674026489258\n",
            "989 -> 2.518458366394043 0.694307804107666\n",
            "999 -> 2.619724750518799 0.6947512626647949\n"
          ]
        }
      ],
      "source": [
        "BATCH_IN_SEQUENCES = 384\n",
        "SEQUENCE_LENGTH = 128\n",
        "\n",
        "VOCAB_DIM = 256\n",
        "EMBED_DIM = 512\n",
        "FF_DIM = 2048\n",
        "\n",
        "NUM_HEADS = 8\n",
        "HEAD_DIM = 128\n",
        "\n",
        "LAYERS = 2\n",
        "\n",
        "HEAD_DEPTH = 128\n",
        "\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "FSDP = 8\n",
        "TENSOR = 1\n",
        "\n",
        "\n",
        "def attention_ourselves(_Q, _K, _V):\n",
        "    _weights_unnormalized = jax.numpy.einsum(\"BSHD,BTHD->BHST\", _Q, _K)\n",
        "    _weights_unnormalized_to_zero_out = jax.numpy.triu( jax.numpy.ones((SEQUENCE_LENGTH,SEQUENCE_LENGTH), jax.numpy.bfloat16), 1)\n",
        "    _weights = jax.nn.softmax(_weights_unnormalized - 1e6 * _weights_unnormalized_to_zero_out)  ### Creating something of size (B,HEADS, SEQUENCE, SEQUENCE)\n",
        "    #print(f\"{_weights.size=}\")\n",
        "    output = jax.numpy.einsum(\"BHST,BTHD->BSHD\", _weights, _V)\n",
        "\n",
        "    return output\n",
        "\n",
        "class OurModel(nn.Module):\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    '''\n",
        "        x is [BATCH, SEQUENCE]\n",
        "    '''\n",
        "    embedding = self.param(\n",
        "        'embedding',\n",
        "        nn.with_partitioning(nn.initializers.normal(1), (\"tp\", \"fsdp\")),\n",
        "        (VOCAB_DIM, EMBED_DIM),\n",
        "        jnp.float32,\n",
        "    )\n",
        "    x = embedding[x] ##OUTPUT should be [BATCH, SEQUENCE, EMBED]\n",
        "\n",
        "    positional_embedding = self.param(\n",
        "        'positional_embedding',\n",
        "        nn.with_partitioning(nn.initializers.normal(1), (None, None, \"fsdp\")),\n",
        "        (1, SEQUENCE_LENGTH, EMBED_DIM),\n",
        "        jnp.float32,\n",
        "    )\n",
        "\n",
        "    x += positional_embedding\n",
        "\n",
        "\n",
        "    for i in range(LAYERS):\n",
        "      feedforward = self.param(\n",
        "          'feedforward_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('fsdp', 'tp')),\n",
        "          (EMBED_DIM, FF_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      x = x @ feedforward\n",
        "      x = jax.nn.relu(x)\n",
        "      embed = self.param(\n",
        "          'embed_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('tp', 'fsdp')),\n",
        "          (FF_DIM, EMBED_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      x = x @ embed\n",
        "      x = jax.nn.relu(x)\n",
        "\n",
        "      q_proj = self.param(\n",
        "          'qproj_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('fsdp', 'tp')),\n",
        "          (EMBED_DIM, NUM_HEADS, HEAD_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      q = jnp.einsum(\"BSE,EHD->BSHD\",x, q_proj )\n",
        "\n",
        "      k_proj = self.param(\n",
        "          'kproj_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('fsdp', 'tp')),\n",
        "          (EMBED_DIM, NUM_HEADS, HEAD_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      k = jnp.einsum(\"BSE,EHD->BSHD\",x, k_proj )\n",
        "\n",
        "      v_proj = self.param(\n",
        "          'vproj_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('fsdp', 'tp')),\n",
        "          (EMBED_DIM, NUM_HEADS, HEAD_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      v = jnp.einsum(\"BSE,EHD->BSHD\",x, v_proj )\n",
        "\n",
        "      o = attention_ourselves(q,k,v)\n",
        "\n",
        "      o_proj = self.param(\n",
        "          'oproj_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('fsdp', 'tp')),\n",
        "          (NUM_HEADS, HEAD_DIM, EMBED_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      x = jnp.einsum(\"BSHD,HDE->BSE\",o, o_proj )\n",
        "\n",
        "    return x @ embedding.T # missing the softmax with probabilities for each token?\n",
        "\n",
        "def convert_to_ascii(string_array, max_length):\n",
        "  result = np.zeros((len(string_array), max_length), dtype=np.uint8)\n",
        "  for i, string in enumerate(string_array):\n",
        "    for j, char in enumerate(string):\n",
        "      if j >= SEQUENCE_LENGTH:\n",
        "         break\n",
        "      result[i, j] = char\n",
        "  return result\n",
        "\n",
        "def input_to_output(np_array):\n",
        "   zero_array = np.zeros( (BATCH_IN_SEQUENCES,SEQUENCE_LENGTH), dtype = jnp.uint8)\n",
        "   zero_array[:, 1:SEQUENCE_LENGTH] = np_array[:, 0:SEQUENCE_LENGTH-1]\n",
        "   return zero_array\n",
        "\n",
        "def calculate_loss(params, model, inputs, outputs):\n",
        "   proposed_outputs = model.apply(params, inputs)\n",
        "   one_hot = jax.nn.one_hot(outputs, VOCAB_DIM)\n",
        "   loss = optax.softmax_cross_entropy(proposed_outputs, one_hot)\n",
        "   return jnp.mean(loss)\n",
        "\n",
        "\n",
        "def step(state, model, inputs, outputs):\n",
        "   loss, grad = jax.value_and_grad(calculate_loss)(state.params, model, inputs, outputs)\n",
        "   state = state.apply_gradients(grads = grad)\n",
        "   return loss, state\n",
        "\n",
        "\n",
        "mesh = jax.sharding.Mesh(np.reshape(  jax.devices(), (FSDP,TENSOR)), [\"fsdp\", \"tp\"])\n",
        "\n",
        "ds = tfds.load('lm1b', split='train', shuffle_files=False)\n",
        "ds = ds.batch(BATCH_IN_SEQUENCES)\n",
        "\n",
        "rngkey = jax.random.key(0)\n",
        "model = OurModel()\n",
        "\n",
        "shaped_init = jax.eval_shape( functools.partial(model.init, rngkey), jax.ShapeDtypeStruct((BATCH_IN_SEQUENCES, SEQUENCE_LENGTH), dtype = jnp.uint8))\n",
        "state_sharding = nn.get_sharding(shaped_init, mesh)\n",
        "_params = jax.jit(model.init, out_shardings = state_sharding)(rngkey, jax.ShapeDtypeStruct((BATCH_IN_SEQUENCES, SEQUENCE_LENGTH), dtype = jnp.uint8))\n",
        "\n",
        "tx = optax.adam(learning_rate = LEARNING_RATE)\n",
        "state = train_state.TrainState.create(\n",
        "    apply_fn = model.apply,\n",
        "    params = _params,\n",
        "    tx = tx\n",
        ")\n",
        "\n",
        "iter = 0\n",
        "static_step = jax.jit(step, static_argnums=1)\n",
        "\n",
        "last_step_time = time.time()\n",
        "stepnum = 0\n",
        "\n",
        "for example in ds:\n",
        "    outputs = convert_to_ascii(example['text'].numpy(), SEQUENCE_LENGTH)\n",
        "    inputs = input_to_output(outputs)\n",
        "\n",
        "    loss, state = static_step(state, model, inputs, outputs)\n",
        "    # assert model_without_pos_embeddings(\"Sam and Erin\") == model_without_pos_embeddings(\"Erin and Sam\")\n",
        "    # assert model_with_pos_embeddings(\"Sam and Erin\") != model_with_pos_embeddings(\"Erin and Sam\")\n",
        "    #loss, state = jax.jit(step, static_argnums=1)(state, model, inputs, outputs)\n",
        "    #loss, state = jax.jit(lambda x,y,z,a : step(x,y,z,a), static_argnums=1)(state, model, inputs, outputs)\n",
        "\n",
        "    stepnum += 1\n",
        "\n",
        "    if stepnum % 10 == 0:\n",
        "      new_time = time.time()\n",
        "      time_elapsed_seconds = (new_time-last_step_time)\n",
        "      last_step_time = new_time\n",
        "      print(f\"{iter} -> {loss} {time_elapsed_seconds}\")\n",
        "\n",
        "\n",
        "    iter += 1\n",
        "    if stepnum == 1000:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhmffN3ZKCZ1",
        "outputId": "143d6962-0d31-456c-903b-e3d2c0610151"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9 -> 4.641420364379883 10.499782800674438\n",
            "19 -> 3.4807138442993164 0.4404611587524414\n",
            "29 -> 3.126253604888916 0.4737560749053955\n",
            "39 -> 2.970153570175171 0.4640030860900879\n",
            "49 -> 2.615723133087158 0.4700591564178467\n",
            "59 -> 2.5520169734954834 0.49142980575561523\n",
            "69 -> 2.589102268218994 0.4575340747833252\n",
            "79 -> 2.584998607635498 0.46743297576904297\n",
            "89 -> 2.5628955364227295 0.4687764644622803\n",
            "99 -> 2.529839277267456 0.46895742416381836\n",
            "109 -> 2.508159637451172 0.4701712131500244\n",
            "119 -> 2.8094897270202637 0.4691286087036133\n",
            "129 -> 3.0647389888763428 0.4703054428100586\n",
            "139 -> 2.7343554496765137 0.46842312812805176\n",
            "149 -> 2.574105978012085 0.4739658832550049\n",
            "159 -> 2.5387516021728516 0.46845340728759766\n",
            "169 -> 2.547395706176758 0.46853137016296387\n",
            "179 -> 2.5696353912353516 0.47060084342956543\n",
            "189 -> 2.5902185440063477 0.4735286235809326\n",
            "199 -> 2.4601614475250244 0.47171998023986816\n",
            "209 -> 2.4510066509246826 0.4686911106109619\n",
            "219 -> 2.786611318588257 0.4655430316925049\n",
            "229 -> 2.488659381866455 0.4765346050262451\n",
            "239 -> 2.460153818130493 0.4636116027832031\n",
            "249 -> 2.3947081565856934 0.4689650535583496\n",
            "259 -> 2.341618061065674 0.47066760063171387\n",
            "269 -> 2.329960823059082 0.4698171615600586\n",
            "279 -> 2.2935194969177246 0.46932363510131836\n",
            "289 -> 2.276033878326416 0.469437837600708\n",
            "299 -> 2.287144660949707 0.4698758125305176\n",
            "309 -> 2.263576030731201 0.4687626361846924\n",
            "319 -> 2.2820096015930176 0.46929144859313965\n",
            "329 -> 2.2188186645507812 0.47005248069763184\n",
            "339 -> 2.2022526264190674 0.4695136547088623\n",
            "349 -> 2.3131837844848633 0.47026491165161133\n",
            "359 -> 2.2354111671447754 0.4697103500366211\n",
            "369 -> 2.2945809364318848 0.47368383407592773\n",
            "379 -> 2.202528953552246 0.46643757820129395\n",
            "389 -> 2.236464500427246 0.46833276748657227\n",
            "399 -> 2.209191083908081 0.47264528274536133\n",
            "409 -> 2.17922043800354 0.4668591022491455\n",
            "419 -> 2.1674723625183105 0.469895601272583\n",
            "429 -> 2.2459561824798584 0.4696540832519531\n",
            "439 -> 2.2202484607696533 0.4683382511138916\n",
            "449 -> 2.1478753089904785 0.47520017623901367\n",
            "459 -> 2.113795757293701 0.4669475555419922\n",
            "469 -> 2.155224084854126 0.46944212913513184\n",
            "479 -> 2.153573513031006 0.4708383083343506\n",
            "489 -> 2.1399660110473633 0.4703836441040039\n",
            "499 -> 2.095996618270874 0.46961426734924316\n",
            "509 -> 2.101815700531006 0.46947693824768066\n",
            "519 -> 2.0810251235961914 0.46959567070007324\n",
            "529 -> 2.1589560508728027 0.46950316429138184\n",
            "539 -> 2.109182357788086 0.476010799407959\n",
            "549 -> 2.0584566593170166 0.464902400970459\n",
            "559 -> 2.1271355152130127 0.4698023796081543\n",
            "569 -> 2.143247604370117 0.4764244556427002\n",
            "579 -> 2.1776797771453857 0.4652574062347412\n",
            "589 -> 2.083224296569824 0.4734811782836914\n",
            "599 -> 2.108607053756714 0.46629810333251953\n",
            "609 -> 2.109079599380493 0.46982526779174805\n",
            "619 -> 2.0618743896484375 0.4715547561645508\n",
            "629 -> 2.1045868396759033 0.46863293647766113\n",
            "639 -> 2.1411311626434326 0.47106099128723145\n",
            "649 -> 2.0902886390686035 0.4666874408721924\n",
            "659 -> 2.1539793014526367 0.4717118740081787\n",
            "669 -> 2.151933193206787 0.47388553619384766\n",
            "679 -> 2.081521511077881 0.47004175186157227\n",
            "689 -> 2.0784943103790283 0.4687919616699219\n",
            "699 -> 2.106808662414551 0.46953272819519043\n",
            "709 -> 2.0744740962982178 0.47179198265075684\n",
            "719 -> 2.083129644393921 0.4688260555267334\n",
            "729 -> 2.058112621307373 0.46900033950805664\n",
            "739 -> 2.091292381286621 0.4735689163208008\n",
            "749 -> 2.059067487716675 0.46643733978271484\n",
            "759 -> 2.0949251651763916 0.4692230224609375\n",
            "769 -> 2.0048940181732178 0.4692683219909668\n",
            "779 -> 2.0072412490844727 0.4695463180541992\n",
            "789 -> 2.0421104431152344 0.4699056148529053\n",
            "799 -> 2.0156161785125732 0.4692564010620117\n",
            "809 -> 2.043156623840332 0.47080397605895996\n",
            "819 -> 2.0076637268066406 0.46891164779663086\n",
            "829 -> 2.007058620452881 0.46834254264831543\n",
            "839 -> 2.0736615657806396 0.4706306457519531\n",
            "849 -> 2.0162265300750732 0.4701557159423828\n",
            "859 -> 1.9950454235076904 0.4684028625488281\n",
            "869 -> 2.0021185874938965 0.47333264350891113\n",
            "879 -> 2.1513497829437256 0.46965551376342773\n",
            "889 -> 2.037083864212036 0.4678988456726074\n",
            "899 -> 2.032519817352295 0.46826744079589844\n",
            "909 -> 2.0362343788146973 0.4705851078033447\n",
            "919 -> 2.072342872619629 0.46914076805114746\n",
            "929 -> 2.0977396965026855 0.4745466709136963\n",
            "939 -> 2.0317893028259277 0.4658534526824951\n",
            "949 -> 2.045194149017334 0.4702298641204834\n",
            "959 -> 2.0102972984313965 0.46918725967407227\n",
            "969 -> 2.0299365520477295 0.4698445796966553\n",
            "979 -> 2.0215368270874023 0.47191405296325684\n",
            "989 -> 1.96022367477417 0.46686387062072754\n",
            "999 -> 2.046095848083496 0.469912052154541\n"
          ]
        }
      ],
      "source": [
        "BATCH_IN_SEQUENCES = 384\n",
        "SEQUENCE_LENGTH = 128\n",
        "\n",
        "VOCAB_DIM = 256\n",
        "EMBED_DIM = 512\n",
        "FF_DIM = 2048\n",
        "\n",
        "NUM_HEADS = 4\n",
        "HEAD_DIM = 128\n",
        "\n",
        "LAYERS = 2\n",
        "\n",
        "HEAD_DEPTH = 128\n",
        "\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "FSDP = 4\n",
        "TENSOR = 2\n",
        "\n",
        "\n",
        "def attention_ourselves(_Q, _K, _V):\n",
        "    _weights_unnormalized = jax.numpy.einsum(\"BSHD,BTHD->BHST\", _Q, _K)\n",
        "    _weights_unnormalized_to_zero_out = jax.numpy.triu( jax.numpy.ones((SEQUENCE_LENGTH,SEQUENCE_LENGTH), jax.numpy.bfloat16), 1)\n",
        "    _weights = jax.nn.softmax(_weights_unnormalized - 1e6 * _weights_unnormalized_to_zero_out)  ### Creating something of size (B,HEADS, SEQUENCE, SEQUENCE)\n",
        "    #print(f\"{_weights.size=}\")\n",
        "    output = jax.numpy.einsum(\"BHST,BTHD->BSHD\", _weights, _V)\n",
        "\n",
        "    return output\n",
        "\n",
        "class OurModel(nn.Module):\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    '''\n",
        "        x is [BATCH, SEQUENCE]\n",
        "    '''\n",
        "    embedding = self.param(\n",
        "        'embedding',\n",
        "        nn.with_partitioning(nn.initializers.normal(1), (\"tp\", \"fsdp\")),\n",
        "        (VOCAB_DIM, EMBED_DIM),\n",
        "        jnp.float32,\n",
        "    )\n",
        "    x = embedding[x] ##OUTPUT should be [BATCH, SEQUENCE, EMBED]\n",
        "\n",
        "    positional_embedding = self.param(\n",
        "        'positional_embedding',\n",
        "        nn.with_partitioning(nn.initializers.normal(1), (None, None, \"fsdp\")),\n",
        "        (1, SEQUENCE_LENGTH, EMBED_DIM),\n",
        "        jnp.float32,\n",
        "    )\n",
        "\n",
        "    x += positional_embedding\n",
        "\n",
        "\n",
        "    for i in range(LAYERS):\n",
        "      feedforward = self.param(\n",
        "          'feedforward_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('fsdp', 'tp')),\n",
        "          (EMBED_DIM, FF_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      x = x @ feedforward\n",
        "      x = jax.nn.relu(x)\n",
        "      embed = self.param(\n",
        "          'embed_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('tp', 'fsdp')),\n",
        "          (FF_DIM, EMBED_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      x = x @ embed\n",
        "      x = jax.nn.relu(x)\n",
        "\n",
        "      q_proj = self.param(\n",
        "          'qproj_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('fsdp', 'tp')),\n",
        "          (EMBED_DIM, NUM_HEADS, HEAD_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      q = jnp.einsum(\"BSE,EHD->BSHD\",x, q_proj )\n",
        "\n",
        "      k_proj = self.param(\n",
        "          'kproj_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('fsdp', 'tp')),\n",
        "          (EMBED_DIM, NUM_HEADS, HEAD_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      k = jnp.einsum(\"BSE,EHD->BSHD\",x, k_proj )\n",
        "\n",
        "      v_proj = self.param(\n",
        "          'vproj_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('fsdp', 'tp')),\n",
        "          (EMBED_DIM, NUM_HEADS, HEAD_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      v = jnp.einsum(\"BSE,EHD->BSHD\",x, v_proj )\n",
        "\n",
        "      o = attention_ourselves(q,k,v)\n",
        "\n",
        "      o_proj = self.param(\n",
        "          'oproj_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('fsdp', 'tp')),\n",
        "          (NUM_HEADS, HEAD_DIM, EMBED_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      x = jnp.einsum(\"BSHD,HDE->BSE\",o, o_proj )\n",
        "\n",
        "    return x @ embedding.T # missing the softmax with probabilities for each token?\n",
        "\n",
        "def convert_to_ascii(string_array, max_length):\n",
        "  result = np.zeros((len(string_array), max_length), dtype=np.uint8)\n",
        "  for i, string in enumerate(string_array):\n",
        "    for j, char in enumerate(string):\n",
        "      if j >= SEQUENCE_LENGTH:\n",
        "         break\n",
        "      result[i, j] = char\n",
        "  return result\n",
        "\n",
        "def input_to_output(np_array):\n",
        "   zero_array = np.zeros( (BATCH_IN_SEQUENCES,SEQUENCE_LENGTH), dtype = jnp.uint8)\n",
        "   zero_array[:, 1:SEQUENCE_LENGTH] = np_array[:, 0:SEQUENCE_LENGTH-1]\n",
        "   return zero_array\n",
        "\n",
        "def calculate_loss(params, model, inputs, outputs):\n",
        "   proposed_outputs = model.apply(params, inputs)\n",
        "   one_hot = jax.nn.one_hot(outputs, VOCAB_DIM)\n",
        "   loss = optax.softmax_cross_entropy(proposed_outputs, one_hot)\n",
        "   return jnp.mean(loss)\n",
        "\n",
        "\n",
        "def step(state, model, inputs, outputs):\n",
        "   loss, grad = jax.value_and_grad(calculate_loss)(state.params, model, inputs, outputs)\n",
        "   state = state.apply_gradients(grads = grad)\n",
        "   return loss, state\n",
        "\n",
        "\n",
        "mesh = jax.sharding.Mesh(np.reshape(  jax.devices(), (FSDP,TENSOR)), [\"fsdp\", \"tp\"])\n",
        "\n",
        "ds = tfds.load('lm1b', split='train', shuffle_files=False)\n",
        "ds = ds.batch(BATCH_IN_SEQUENCES)\n",
        "\n",
        "rngkey = jax.random.key(0)\n",
        "model = OurModel()\n",
        "\n",
        "shaped_init = jax.eval_shape( functools.partial(model.init, rngkey), jax.ShapeDtypeStruct((BATCH_IN_SEQUENCES, SEQUENCE_LENGTH), dtype = jnp.uint8))\n",
        "state_sharding = nn.get_sharding(shaped_init, mesh)\n",
        "_params = jax.jit(model.init, out_shardings = state_sharding)(rngkey, jax.ShapeDtypeStruct((BATCH_IN_SEQUENCES, SEQUENCE_LENGTH), dtype = jnp.uint8))\n",
        "\n",
        "tx = optax.adam(learning_rate = LEARNING_RATE)\n",
        "state = train_state.TrainState.create(\n",
        "    apply_fn = model.apply,\n",
        "    params = _params,\n",
        "    tx = tx\n",
        ")\n",
        "\n",
        "iter = 0\n",
        "static_step = jax.jit(step, static_argnums=1)\n",
        "\n",
        "last_step_time = time.time()\n",
        "stepnum = 0\n",
        "\n",
        "for example in ds:\n",
        "    outputs = convert_to_ascii(example['text'].numpy(), SEQUENCE_LENGTH)\n",
        "    inputs = input_to_output(outputs)\n",
        "\n",
        "    loss, state = static_step(state, model, inputs, outputs)\n",
        "    # assert model_without_pos_embeddings(\"Sam and Erin\") == model_without_pos_embeddings(\"Erin and Sam\")\n",
        "    # assert model_with_pos_embeddings(\"Sam and Erin\") != model_with_pos_embeddings(\"Erin and Sam\")\n",
        "    #loss, state = jax.jit(step, static_argnums=1)(state, model, inputs, outputs)\n",
        "    #loss, state = jax.jit(lambda x,y,z,a : step(x,y,z,a), static_argnums=1)(state, model, inputs, outputs)\n",
        "\n",
        "    stepnum += 1\n",
        "\n",
        "    if stepnum % 10 == 0:\n",
        "      new_time = time.time()\n",
        "      time_elapsed_seconds = (new_time-last_step_time)\n",
        "      last_step_time = new_time\n",
        "      print(f\"{iter} -> {loss} {time_elapsed_seconds}\")\n",
        "\n",
        "\n",
        "    iter += 1\n",
        "    if stepnum == 1000:\n",
        "      break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeG5iDNrisOl",
        "outputId": "10b0693c-ca32-41e7-edc2-35a77b454b88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 128, 256)\n",
            "logits 1 and 2 are the same: True\n",
            "logits 1 and 3 are NOT the same: True\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def predict(input_str, model, params):\n",
        "    input_tf = tf.constant([input_str], shape=(1,), dtype=tf.string)\n",
        "    input_ascii = convert_to_ascii(input_tf.numpy(), SEQUENCE_LENGTH)\n",
        "    logits = model.apply(params, input_ascii)\n",
        "    return logits\n",
        "\n",
        "# for example in ds:\n",
        "#     print(example[\"text\"][1:2])\n",
        "#     inputs = convert_to_ascii(example['text'][1:2].numpy(), SEQUENCE_LENGTH)\n",
        "#     break\n",
        "logits_1 = predict(\"test a\", model, state.params)\n",
        "logits_2 = predict(\"test a\", model, state.params)\n",
        "logits_3 = predict(\"a test\", model, state.params)\n",
        "print(logits.shape)\n",
        "print(\"logits 1 and 2 are the same:\", jnp.array_equal(logits_1, logits_2))\n",
        "print(\"logits 1 and 3 are NOT the same:\", not jnp.array_equal(logits_1, logits_3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Remove position embedding and rerun check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9 -> 3.738297939300537 10.284725666046143\n",
            "19 -> 3.2976555824279785 0.4366028308868408\n",
            "29 -> 2.9179940223693848 0.46828627586364746\n",
            "39 -> 2.6275124549865723 0.4704246520996094\n",
            "49 -> 2.5893445014953613 0.4669492244720459\n",
            "59 -> 2.526007890701294 0.4689810276031494\n",
            "69 -> 2.565831422805786 0.46811962127685547\n",
            "79 -> 2.552938461303711 0.4696502685546875\n",
            "89 -> 2.4898769855499268 0.4689011573791504\n",
            "99 -> 2.4497549533843994 0.4737987518310547\n",
            "109 -> 2.3568036556243896 0.4676961898803711\n",
            "119 -> 2.33327579498291 0.4705238342285156\n",
            "129 -> 2.2718138694763184 0.4678027629852295\n",
            "139 -> 2.289827823638916 0.46984219551086426\n",
            "149 -> 2.197004795074463 0.467543363571167\n",
            "159 -> 2.1709423065185547 0.4693896770477295\n",
            "169 -> 2.17602276802063 0.4683966636657715\n",
            "179 -> 2.1939661502838135 0.47025537490844727\n",
            "189 -> 2.231370687484741 0.464611291885376\n",
            "199 -> 2.1232893466949463 0.4722745418548584\n",
            "209 -> 2.1250648498535156 0.4691329002380371\n",
            "219 -> 2.1651811599731445 0.46866273880004883\n",
            "229 -> 2.094897985458374 0.4681859016418457\n",
            "239 -> 2.1238837242126465 0.46948862075805664\n",
            "249 -> 2.1162924766540527 0.4668309688568115\n",
            "259 -> 2.088003158569336 0.4714789390563965\n",
            "269 -> 2.08986759185791 0.4689669609069824\n",
            "279 -> 2.060004234313965 0.46718311309814453\n",
            "289 -> 2.0679726600646973 0.4661240577697754\n",
            "299 -> 2.094775676727295 0.4691131114959717\n",
            "309 -> 2.0683939456939697 0.4678683280944824\n",
            "319 -> 2.0899674892425537 0.47019457817077637\n",
            "329 -> 2.0322189331054688 0.4707005023956299\n",
            "339 -> 2.0262017250061035 0.469376802444458\n",
            "349 -> 2.042421817779541 0.4702260494232178\n",
            "359 -> 2.057363986968994 0.4677712917327881\n",
            "369 -> 2.114412546157837 0.4691176414489746\n",
            "379 -> 2.0345852375030518 0.47283339500427246\n",
            "389 -> 2.0705103874206543 0.47384214401245117\n",
            "399 -> 2.043822765350342 0.46805334091186523\n",
            "409 -> 2.0316007137298584 0.46663379669189453\n",
            "419 -> 2.0310587882995605 0.4701118469238281\n",
            "429 -> 2.0240137577056885 0.4695148468017578\n",
            "439 -> 2.031094551086426 0.4689953327178955\n",
            "449 -> 2.0014331340789795 0.4703543186187744\n",
            "459 -> 1.9809540510177612 0.4675581455230713\n",
            "469 -> 2.026212453842163 0.46921443939208984\n",
            "479 -> 2.027972459793091 0.4694373607635498\n",
            "489 -> 2.0204944610595703 0.4683716297149658\n",
            "499 -> 1.9798959493637085 0.46967649459838867\n",
            "509 -> 1.9873759746551514 0.4691603183746338\n",
            "519 -> 1.9649604558944702 0.46990036964416504\n",
            "529 -> 2.0375277996063232 0.46800875663757324\n",
            "539 -> 1.997972846031189 0.46729063987731934\n",
            "549 -> 1.9615992307662964 0.47017526626586914\n",
            "559 -> 2.01446533203125 0.4706919193267822\n",
            "569 -> 2.0488622188568115 0.4706254005432129\n",
            "579 -> 1.9743461608886719 0.46947813034057617\n",
            "589 -> 1.9878263473510742 0.4688076972961426\n",
            "599 -> 1.9951848983764648 0.4674336910247803\n",
            "609 -> 1.9962924718856812 0.47165870666503906\n",
            "619 -> 1.9693126678466797 0.4680330753326416\n",
            "629 -> 2.0226967334747314 0.4697139263153076\n",
            "639 -> 2.0134572982788086 0.467862606048584\n",
            "649 -> 1.953440546989441 0.46933913230895996\n",
            "659 -> 2.034846305847168 0.47012829780578613\n",
            "669 -> 2.045515537261963 0.4683976173400879\n",
            "679 -> 1.9835035800933838 0.46936941146850586\n",
            "689 -> 1.9872756004333496 0.47004175186157227\n",
            "699 -> 2.0045769214630127 0.46769165992736816\n",
            "709 -> 1.9876145124435425 0.46642208099365234\n",
            "719 -> 2.001133918762207 0.4726676940917969\n",
            "729 -> 1.9711116552352905 0.4691500663757324\n",
            "739 -> 1.9870976209640503 0.4693288803100586\n",
            "749 -> 1.976869821548462 0.46810364723205566\n",
            "759 -> 1.993277907371521 0.46524977684020996\n",
            "769 -> 1.926316261291504 0.4722259044647217\n",
            "779 -> 1.9263118505477905 0.46881818771362305\n",
            "789 -> 1.9638570547103882 0.4654691219329834\n",
            "799 -> 1.9348410367965698 0.4702458381652832\n",
            "809 -> 1.9509752988815308 0.4669990539550781\n",
            "819 -> 1.9151639938354492 0.46910977363586426\n",
            "829 -> 1.9747376441955566 0.47100305557250977\n",
            "839 -> 2.0008373260498047 0.47150683403015137\n",
            "849 -> 1.9299935102462769 0.46947574615478516\n",
            "859 -> 1.9326622486114502 0.4684627056121826\n",
            "869 -> 1.9144346714019775 0.4675264358520508\n",
            "879 -> 2.0149614810943604 0.4702622890472412\n",
            "889 -> 1.9467514753341675 0.46901655197143555\n",
            "899 -> 1.9549320936203003 0.4680187702178955\n",
            "909 -> 1.9582390785217285 0.46967196464538574\n",
            "919 -> 1.9970042705535889 0.4703497886657715\n",
            "929 -> 1.9974445104599 0.4683394432067871\n",
            "939 -> 1.9540356397628784 0.467801570892334\n",
            "949 -> 1.9714293479919434 0.4695577621459961\n",
            "959 -> 1.9472159147262573 0.4701957702636719\n",
            "969 -> 1.962263584136963 0.46869421005249023\n",
            "979 -> 1.9488657712936401 0.4684116840362549\n",
            "989 -> 1.8889917135238647 0.47141551971435547\n",
            "999 -> 1.9706144332885742 0.4688708782196045\n"
          ]
        }
      ],
      "source": [
        "BATCH_IN_SEQUENCES = 384\n",
        "SEQUENCE_LENGTH = 128\n",
        "\n",
        "VOCAB_DIM = 256\n",
        "EMBED_DIM = 512\n",
        "FF_DIM = 2048\n",
        "\n",
        "NUM_HEADS = 4\n",
        "HEAD_DIM = 128\n",
        "\n",
        "LAYERS = 2\n",
        "\n",
        "HEAD_DEPTH = 128\n",
        "\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "FSDP = 4\n",
        "TENSOR = 2\n",
        "\n",
        "\n",
        "def attention_ourselves(_Q, _K, _V):\n",
        "    _weights_unnormalized = jax.numpy.einsum(\"BSHD,BTHD->BHST\", _Q, _K)\n",
        "    _weights_unnormalized_to_zero_out = jax.numpy.triu( jax.numpy.ones((SEQUENCE_LENGTH,SEQUENCE_LENGTH), jax.numpy.bfloat16), 1)\n",
        "    _weights = jax.nn.softmax(_weights_unnormalized - 1e6 * _weights_unnormalized_to_zero_out)  ### Creating something of size (B,HEADS, SEQUENCE, SEQUENCE)\n",
        "    #print(f\"{_weights.size=}\")\n",
        "    output = jax.numpy.einsum(\"BHST,BTHD->BSHD\", _weights, _V)\n",
        "\n",
        "    return output\n",
        "\n",
        "class OurModel(nn.Module):\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    '''\n",
        "        x is [BATCH, SEQUENCE]\n",
        "    '''\n",
        "    embedding = self.param(\n",
        "        'embedding',\n",
        "        nn.with_partitioning(nn.initializers.normal(1), (\"tp\", \"fsdp\")),\n",
        "        (VOCAB_DIM, EMBED_DIM),\n",
        "        jnp.float32,\n",
        "    )\n",
        "    x = embedding[x] ##OUTPUT should be [BATCH, SEQUENCE, EMBED]\n",
        "\n",
        "    # positional_embedding = self.param(\n",
        "    #     'positional_embedding',\n",
        "    #     nn.with_partitioning(nn.initializers.normal(1), (None, None, \"fsdp\")),\n",
        "    #     (1, SEQUENCE_LENGTH, EMBED_DIM),\n",
        "    #     jnp.float32,\n",
        "    # )\n",
        "\n",
        "    # x += positional_embedding\n",
        "\n",
        "\n",
        "    for i in range(LAYERS):\n",
        "      feedforward = self.param(\n",
        "          'feedforward_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('fsdp', 'tp')),\n",
        "          (EMBED_DIM, FF_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      x = x @ feedforward\n",
        "      x = jax.nn.relu(x)\n",
        "      embed = self.param(\n",
        "          'embed_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('tp', 'fsdp')),\n",
        "          (FF_DIM, EMBED_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      x = x @ embed\n",
        "      x = jax.nn.relu(x)\n",
        "\n",
        "      q_proj = self.param(\n",
        "          'qproj_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('fsdp', 'tp')),\n",
        "          (EMBED_DIM, NUM_HEADS, HEAD_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      q = jnp.einsum(\"BSE,EHD->BSHD\",x, q_proj )\n",
        "\n",
        "      k_proj = self.param(\n",
        "          'kproj_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('fsdp', 'tp')),\n",
        "          (EMBED_DIM, NUM_HEADS, HEAD_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      k = jnp.einsum(\"BSE,EHD->BSHD\",x, k_proj )\n",
        "\n",
        "      v_proj = self.param(\n",
        "          'vproj_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('fsdp', 'tp')),\n",
        "          (EMBED_DIM, NUM_HEADS, HEAD_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      v = jnp.einsum(\"BSE,EHD->BSHD\",x, v_proj )\n",
        "\n",
        "      o = attention_ourselves(q,k,v)\n",
        "\n",
        "      o_proj = self.param(\n",
        "          'oproj_' + str(i),\n",
        "          nn.with_partitioning(nn.initializers.lecun_normal(), ('fsdp', 'tp')),\n",
        "          (NUM_HEADS, HEAD_DIM, EMBED_DIM),\n",
        "          jnp.float32,\n",
        "      )\n",
        "      x = jnp.einsum(\"BSHD,HDE->BSE\",o, o_proj )\n",
        "\n",
        "    return x @ embedding.T # missing the softmax with probabilities for each token?\n",
        "\n",
        "def convert_to_ascii(string_array, max_length):\n",
        "  result = np.zeros((len(string_array), max_length), dtype=np.uint8)\n",
        "  for i, string in enumerate(string_array):\n",
        "    for j, char in enumerate(string):\n",
        "      if j >= SEQUENCE_LENGTH:\n",
        "         break\n",
        "      result[i, j] = char\n",
        "  return result\n",
        "\n",
        "def input_to_output(np_array):\n",
        "   zero_array = np.zeros( (BATCH_IN_SEQUENCES,SEQUENCE_LENGTH), dtype = jnp.uint8)\n",
        "   zero_array[:, 1:SEQUENCE_LENGTH] = np_array[:, 0:SEQUENCE_LENGTH-1]\n",
        "   return zero_array\n",
        "\n",
        "def calculate_loss(params, model, inputs, outputs):\n",
        "   proposed_outputs = model.apply(params, inputs)\n",
        "   one_hot = jax.nn.one_hot(outputs, VOCAB_DIM)\n",
        "   loss = optax.softmax_cross_entropy(proposed_outputs, one_hot)\n",
        "   return jnp.mean(loss)\n",
        "\n",
        "\n",
        "def step(state, model, inputs, outputs):\n",
        "   loss, grad = jax.value_and_grad(calculate_loss)(state.params, model, inputs, outputs)\n",
        "   state = state.apply_gradients(grads = grad)\n",
        "   return loss, state\n",
        "\n",
        "\n",
        "mesh = jax.sharding.Mesh(np.reshape(  jax.devices(), (FSDP,TENSOR)), [\"fsdp\", \"tp\"])\n",
        "\n",
        "ds = tfds.load('lm1b', split='train', shuffle_files=False)\n",
        "ds = ds.batch(BATCH_IN_SEQUENCES)\n",
        "\n",
        "rngkey = jax.random.key(0)\n",
        "model = OurModel()\n",
        "\n",
        "shaped_init = jax.eval_shape( functools.partial(model.init, rngkey), jax.ShapeDtypeStruct((BATCH_IN_SEQUENCES, SEQUENCE_LENGTH), dtype = jnp.uint8))\n",
        "state_sharding = nn.get_sharding(shaped_init, mesh)\n",
        "_params = jax.jit(model.init, out_shardings = state_sharding)(rngkey, jax.ShapeDtypeStruct((BATCH_IN_SEQUENCES, SEQUENCE_LENGTH), dtype = jnp.uint8))\n",
        "\n",
        "tx = optax.adam(learning_rate = LEARNING_RATE)\n",
        "state = train_state.TrainState.create(\n",
        "    apply_fn = model.apply,\n",
        "    params = _params,\n",
        "    tx = tx\n",
        ")\n",
        "\n",
        "iter = 0\n",
        "static_step = jax.jit(step, static_argnums=1)\n",
        "\n",
        "last_step_time = time.time()\n",
        "stepnum = 0\n",
        "\n",
        "for example in ds:\n",
        "    outputs = convert_to_ascii(example['text'].numpy(), SEQUENCE_LENGTH)\n",
        "    inputs = input_to_output(outputs)\n",
        "\n",
        "    loss, state = static_step(state, model, inputs, outputs)\n",
        "    # assert model_without_pos_embeddings(\"Sam and Erin\") == model_without_pos_embeddings(\"Erin and Sam\")\n",
        "    # assert model_with_pos_embeddings(\"Sam and Erin\") != model_with_pos_embeddings(\"Erin and Sam\")\n",
        "    #loss, state = jax.jit(step, static_argnums=1)(state, model, inputs, outputs)\n",
        "    #loss, state = jax.jit(lambda x,y,z,a : step(x,y,z,a), static_argnums=1)(state, model, inputs, outputs)\n",
        "\n",
        "    stepnum += 1\n",
        "\n",
        "    if stepnum % 10 == 0:\n",
        "      new_time = time.time()\n",
        "      time_elapsed_seconds = (new_time-last_step_time)\n",
        "      last_step_time = new_time\n",
        "      print(f\"{iter} -> {loss} {time_elapsed_seconds}\")\n",
        "\n",
        "\n",
        "    iter += 1\n",
        "    if stepnum == 1000:\n",
        "      break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 128, 256)\n",
            "logits 1 and 2 are the same: True\n",
            "logits 1 and 3 are NOT the same: True\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def predict(input_str, model, params):\n",
        "    input_tf = tf.constant([input_str], shape=(1,), dtype=tf.string)\n",
        "    input_ascii = convert_to_ascii(input_tf.numpy(), SEQUENCE_LENGTH)\n",
        "    logits = model.apply(params, input_ascii)\n",
        "    return logits\n",
        "\n",
        "# for example in ds:\n",
        "#     print(example[\"text\"][1:2])\n",
        "#     inputs = convert_to_ascii(example['text'][1:2].numpy(), SEQUENCE_LENGTH)\n",
        "#     break\n",
        "logits_1 = predict(\"test a\", model, state.params)\n",
        "logits_2 = predict(\"test a\", model, state.params)\n",
        "logits_3 = predict(\"a test\", model, state.params)\n",
        "print(logits.shape)\n",
        "print(\"logits 1 and 2 are the same:\", jnp.array_equal(logits_1, logits_2))\n",
        "print(\"logits 1 and 3 are NOT the same:\", not jnp.array_equal(logits_1, logits_3))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
