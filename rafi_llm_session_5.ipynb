{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "w7hv9I_icIdp",
        "outputId": "a64e0b17-72d9-4834-dc97-72a9288d390a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ea5d27fbd48a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# attn_value = pallas_attention.mha_reference(Q, K, V,  segment_ids=None, causal=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_ourselves_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import jax\n",
        "from jax.experimental.pallas.ops.tpu import flash_attention as pallas_attention\n",
        "\n",
        "BATCH = 1\n",
        "HEADS = 1\n",
        "SEQUENCE = 2048\n",
        "HEAD_DIM = 128\n",
        "\n",
        "Q = jax.random.normal( jax.random.key(0), (BATCH, SEQUENCE, HEADS, HEAD_DIM))\n",
        "K = jax.random.normal( jax.random.key(1), (BATCH, SEQUENCE, HEADS, HEAD_DIM))\n",
        "V = jax.random.normal( jax.random.key(2), (BATCH, SEQUENCE, HEADS, HEAD_DIM))\n",
        "\n",
        "def attention_ourselves(_Q, _K, _V):\n",
        "    _weights_unnormalized = jax.numpy.einsum(\"BSHD,BTHD->BHST\", _Q, _K)\n",
        "    _weights_unnormalized_to_zero_out = jax.numpy.triu( jax.numpy.ones((SEQUENCE,SEQUENCE), jax.numpy.bfloat16), 1)\n",
        "    _weights = jax.nn.softmax(_weights_unnormalized - 1e6 * _weights_unnormalized_to_zero_out)  ### Creating something of size (B,HEADS, SEQUENCE, SEQUENCE)\n",
        "    #print(f\"{_weights.size=}\")\n",
        "    output = jax.numpy.einsum(\"BHST,BTHD->BSHD\", _weights, _V)\n",
        "\n",
        "    return output\n",
        "\n",
        "attn_ourselves_value = attention_ourselves(Q,K,V)\n",
        "dropout_rate = 0.1  # Replace with an appropriate value\n",
        "attn_value = pallas_attention.mha_reference(Q, K, V, ab=dropout_rate, segment_ids=None, causal=True)\n",
        "# attn_value = pallas_attention.mha_reference(Q, K, V,  segment_ids=None, causal=True)\n",
        "\n",
        "assert jax.numpy.allclose(attn_ourselves_value, attn_value, atol=1e-1, rtol=1e-1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "from jax.experimental.pallas.ops.tpu import flash_attention as pallas_attention\n",
        "\n",
        "BATCH = 1\n",
        "HEADS = 1\n",
        "SEQUENCE = 2048\n",
        "HEAD_DIM = 128\n",
        "\n",
        "Q = jax.random.normal(jax.random.key(0), (BATCH, SEQUENCE, HEADS, HEAD_DIM))\n",
        "K = jax.random.normal(jax.random.key(1), (BATCH, SEQUENCE, HEADS, HEAD_DIM))\n",
        "V = jax.random.normal(jax.random.key(2), (BATCH, SEQUENCE, HEADS, HEAD_DIM))\n",
        "\n",
        "def attention_ourselves(_Q, _K, _V):\n",
        "    _weights_unnormalized = jax.numpy.einsum(\"BSHD,BTHD->BHST\", _Q, _K)\n",
        "    _weights_unnormalized_to_zero_out = jax.numpy.triu(jax.numpy.ones((SEQUENCE, SEQUENCE), jax.numpy.bfloat16), 1)\n",
        "    _weights = jax.nn.softmax(_weights_unnormalized - 1e6 * _weights_unnormalized_to_zero_out)  ### Creating something of size (B,HEADS, SEQUENCE, SEQUENCE)\n",
        "    # print(f\"{_weights.size=}\")\n",
        "    output = jax.numpy.einsum(\"BHST,BTHD->BSHD\", _weights, _V)\n",
        "    return output\n",
        "\n",
        "attn_ourselves_value = attention_ourselves(Q, K, V)\n",
        "attn_value = pallas_attention.mha_reference(Q, K, V, ab=None, segment_ids=None, causal=True)\n",
        "# attn_value = pallas_attention.mha_reference(Q, K, V, segment_ids=None, causal=True)\n",
        "print(f\"{attention_ourselves=}\")\n",
        "print(f\"{attn_value=}\")\n",
        "\n",
        "print(1e-1)\n",
        "# Relax the tolerances to allow for potential numerical differences\n",
        "# or implementation variations between the two attention functions.\n",
        "assert jax.numpy.allclose(attn_ourselves_value, attn_value, atol=0.1, rtol=0.1), f\"Arrays are not close: \\n{jax.numpy.max(jax.numpy.abs(attn_ourselves_value - attn_value))}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "id": "dpEQmWFri91b",
        "outputId": "e4390d2e-92f9-4380-b705-32348a2a9ffc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention_ourselves=<function attention_ourselves at 0x78aafc6334c0>\n",
            "attn_value=Array([[[[ 0.36057308,  1.2849717 , -0.7387236 , ...,  0.9781729 ,\n",
            "           2.2189548 ,  0.38818341]],\n",
            "\n",
            "        [[-1.5729874 , -0.76189977,  0.6108661 , ...,  0.42959312,\n",
            "          -0.21007903,  2.0072043 ]],\n",
            "\n",
            "        [[-0.7875979 , -0.11790697,  0.38122553, ..., -0.15842435,\n",
            "           0.4373327 , -0.92844707]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.39558995,  0.74515975,  0.20654577, ...,  2.295514  ,\n",
            "           0.30290228, -0.48013103]],\n",
            "\n",
            "        [[-0.91627604, -0.6682399 , -1.6973673 , ...,  0.8714954 ,\n",
            "          -1.0584216 , -0.708561  ]],\n",
            "\n",
            "        [[ 0.60500056,  0.08252969,  1.6498895 , ..., -0.04127837,\n",
            "          -0.15703496, -0.72441936]]]], dtype=float32)\n",
            "0.1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Arrays are not close: \n6.370874404907227",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-57514f75fba8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Relax the tolerances to allow for potential numerical differences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# or implementation variations between the two attention functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_ourselves_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Arrays are not close: \\n{jax.numpy.max(jax.numpy.abs(attn_ourselves_value - attn_value))}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: Arrays are not close: \n6.370874404907227"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fix the non causal by using correct einsum"
      ],
      "metadata": {
        "id": "oYoEa493HTv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "from jax.experimental.pallas.ops.tpu import flash_attention as pallas_attention\n",
        "\n",
        "BATCH = 1\n",
        "HEADS = 1\n",
        "SEQUENCE = 2048\n",
        "HEAD_DIM = 128\n",
        "\n",
        "Q = jax.random.normal(jax.random.key(0), (BATCH, SEQUENCE, HEADS, HEAD_DIM))\n",
        "K = jax.random.normal(jax.random.key(1), (BATCH, SEQUENCE, HEADS, HEAD_DIM))\n",
        "V = jax.random.normal(jax.random.key(2), (BATCH, SEQUENCE, HEADS, HEAD_DIM))\n",
        "\n",
        "def attention_ourselves(_Q, _K, _V):\n",
        "    # Batch Sequence Heads HeadsDimension turns into\n",
        "    # Batch Heads Sequence T=OutputSequence\n",
        "    _weights_unnormalized = jax.numpy.einsum(\"BSHD,BTHD->BHST\", _Q, _K)\n",
        "    _weights = jax.nn.softmax(_weights_unnormalized)  ### Creating something of size (B,HEADS, SEQUENCE, SEQUENCE)\n",
        "    print(f\"{_weights.size=}\")\n",
        "    ## This was wrong in original code, code in github was \"BHST,BTHD->BSHD\"\n",
        "    # Question: Why was that wrong?\n",
        "    output = jax.numpy.einsum(\"BHST,BSHD->BSHD\", _weights, _V)\n",
        "    return output\n",
        "\n",
        "attn_ourselves_value = attention_ourselves(Q, K, V)\n",
        "attn_value = pallas_attention.mha_reference(Q, K, V, ab=None, segment_ids=None, causal=False)\n",
        "# attn_value = pallas_attention.mha_reference(Q, K, V, segment_ids=None, causal=True)\n",
        "print(f\"{attention_ourselves=}\")\n",
        "print(f\"{attn_value=}\")\n",
        "\n",
        "print(1e-1)\n",
        "# Relax the tolerances to allow for potential numerical differences\n",
        "# or implementation variations between the two attention functions.\n",
        "assert jax.numpy.allclose(attn_ourselves_value, attn_value, atol=0.1, rtol=0.1), f\"Arrays are not close: \\n{jax.numpy.max(jax.numpy.abs(attn_ourselves_value - attn_value))}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4enlQV-ovDA6",
        "outputId": "d0223d60-1c85-45e8-e54c-39f480cc3c93"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_weights.size=4194304\n",
            "attention_ourselves=<function attention_ourselves at 0x78a8c6bece00>\n",
            "attn_value=Array([[[[ 0.36057308,  1.2849717 , -0.7387236 , ...,  0.9781729 ,\n",
            "           2.2189548 ,  0.38818341]],\n",
            "\n",
            "        [[-1.5729874 , -0.76189977,  0.6108661 , ...,  0.42959312,\n",
            "          -0.21007903,  2.0072043 ]],\n",
            "\n",
            "        [[-0.7875979 , -0.11790697,  0.38122553, ..., -0.15842435,\n",
            "           0.4373327 , -0.92844707]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.39558995,  0.74515975,  0.20654577, ...,  2.295514  ,\n",
            "           0.30290228, -0.48013103]],\n",
            "\n",
            "        [[-0.91627604, -0.6682399 , -1.6973673 , ...,  0.8714954 ,\n",
            "          -1.0584216 , -0.708561  ]],\n",
            "\n",
            "        [[ 0.60500056,  0.08252969,  1.6498895 , ..., -0.04127837,\n",
            "          -0.15703496, -0.72441936]]]], dtype=float32)\n",
            "0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fix causal"
      ],
      "metadata": {
        "id": "o4aX4I4gHb4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "from jax.experimental.pallas.ops.tpu import flash_attention as pallas_attention\n",
        "\n",
        "BATCH = 1\n",
        "HEADS = 1\n",
        "SEQUENCE = 2048\n",
        "HEAD_DIM = 128\n",
        "\n",
        "Q = jax.random.normal(jax.random.key(0), (BATCH, SEQUENCE, HEADS, HEAD_DIM))\n",
        "K = jax.random.normal(jax.random.key(1), (BATCH, SEQUENCE, HEADS, HEAD_DIM))\n",
        "V = jax.random.normal(jax.random.key(2), (BATCH, SEQUENCE, HEADS, HEAD_DIM))\n",
        "\n",
        "def attention_ourselves(_Q, _K, _V):\n",
        "    _weights_unnormalized = jax.numpy.einsum(\"BSHD,BTHD->BHST\", _Q, _K)\n",
        "    _weights_unnormalized_to_zero_out = jax.numpy.triu( jax.numpy.ones((SEQUENCE,SEQUENCE), jax.numpy.bfloat16), 1)\n",
        "    _weights = jax.nn.softmax(_weights_unnormalized - 1e6 * _weights_unnormalized_to_zero_out)  ### Creating something of size (B,HEADS, SEQUENCE, SEQUENCE)\n",
        "    # weight is Seq x Seq\n",
        "    print(f\"{_weights.size=}\")\n",
        "    ## This was wrong in original code, code in github was \"BHST,BTHD->BSHD\"\n",
        "    # Question: Why was that wrong?\n",
        "    output = jax.numpy.einsum(\"BHST,BSHD->BSHD\", _weights, _V)\n",
        "    return output\n",
        "\n",
        "attn_ourselves_value = attention_ourselves(Q, K, V)\n",
        "attn_value = pallas_attention.mha_reference(Q, K, V, ab=None, segment_ids=None, causal=True)\n",
        "# attn_value = pallas_attention.mha_reference(Q, K, V, segment_ids=None, causal=True)\n",
        "print(f\"{attention_ourselves=}\")\n",
        "print(f\"{attn_value=}\")\n",
        "\n",
        "print(1e-1)\n",
        "# Relax the tolerances to allow for potential numerical differences\n",
        "# or implementation variations between the two attention functions.\n",
        "assert jax.numpy.allclose(attn_ourselves_value, attn_value, atol=0.1, rtol=0.1), f\"Arrays are not close: \\n{jax.numpy.max(jax.numpy.abs(attn_ourselves_value - attn_value))}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SRJzvRKwJsP",
        "outputId": "733d10dc-4a76-43b9-b761-df65c8db4285"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_weights.size=4194304\n",
            "attention_ourselves=<function attention_ourselves at 0x7c3028082a20>\n",
            "attn_value=Array([[[[ 0.36057308,  1.2849717 , -0.7387236 , ...,  0.9781729 ,\n",
            "           2.2189548 ,  0.38818341]],\n",
            "\n",
            "        [[-1.5729874 , -0.76189977,  0.6108661 , ...,  0.42959312,\n",
            "          -0.21007903,  2.0072043 ]],\n",
            "\n",
            "        [[-0.7875979 , -0.11790697,  0.38122553, ..., -0.15842435,\n",
            "           0.4373327 , -0.92844707]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.39558995,  0.74515975,  0.20654577, ...,  2.295514  ,\n",
            "           0.30290228, -0.48013103]],\n",
            "\n",
            "        [[-0.91627604, -0.6682399 , -1.6973673 , ...,  0.8714954 ,\n",
            "          -1.0584216 , -0.708561  ]],\n",
            "\n",
            "        [[ 0.60500056,  0.08252969,  1.6498895 , ..., -0.04127837,\n",
            "          -0.15703496, -0.72441936]]]], dtype=float32)\n",
            "0.1\n"
          ]
        }
      ]
    }
  ]
}